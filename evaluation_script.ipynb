{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "This notebook requires both a Python kernel and an R kernel\n",
    "* [Preperation](#prep)\n",
    "    * [Read in data](#data)\n",
    "* [Sample description](#sample)\n",
    "    * [Education and Age](#eduage)\n",
    "    * [Parental disorder](#dis)\n",
    "    * [Sample Comparison](#samplecom)\n",
    "* [Frequentist Statistic](#frequentist)\n",
    "    * [BoxPlot](#boxplot)\n",
    "    * [KG vs. EG](#vs)\n",
    "    * [Correlation parenting skill](#corr)\n",
    "    * [Correlation parenting stress](#corrstress)\n",
    "    * [Correlation Emotionregulation](#emotion)\n",
    "    * [Correlation Heatmaps](#heat)\n",
    "* [Bayesian Statistic - R Kernel](#bayes)\n",
    "    * [Sample Comparison with R](#samplecomR)\n",
    "    * [KG vs. EG with R](#bayeseg)\n",
    "    * [Correlation parenting skill with R](#corrbayes)\n",
    "    * [Correlation parenting stress with R](#corrstressbayes)\n",
    "    * [Correlation Emotionregulation with R](#emotionbayes)\n",
    "\n",
    "\n",
    "    bayeseg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preperation <a class=\"anchor\" id=\"prep\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from neuroCombat import neuroCombat\n",
    "from scipy.stats import norm\n",
    "import scipy.stats as sc\n",
    "import statsmodels\n",
    "\n",
    "# base code\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from statsmodels.tools.tools import maybe_unwrap_results\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Type\n",
    "\n",
    "\n",
    "class LinearRegDiagnostic():\n",
    "    \"\"\"\n",
    "    Diagnostic plots to identify potential problems in a linear regression fit.\n",
    "    Mainly,\n",
    "        a. non-linearity of data\n",
    "        b. Correlation of error terms\n",
    "        c. non-constant variance\n",
    "        d. outliers\n",
    "        e. high-leverage points\n",
    "        f. collinearity\n",
    "\n",
    "    Authors:\n",
    "        Prajwal Kafle (p33ajkafle@gmail.com, where 3 = r)\n",
    "        Does not come with any sort of warranty.\n",
    "        Please test the code one your end before using.\n",
    "\n",
    "        Matt Spinelli (m3spinelli@gmail.com, where 3 = r)\n",
    "        (1) Fixed incorrect annotation of the top most extreme residuals in\n",
    "            the Residuals vs Fitted and, especially, the Normal Q-Q plots.\n",
    "        (2) Changed Residuals vs Leverage plot to match closer the y-axis\n",
    "            range shown in the equivalent plot in the R package ggfortify.\n",
    "        (3) Added horizontal line at y=0 in Residuals vs Leverage plot to\n",
    "            match the plots in R package ggfortify and base R.\n",
    "        (4) Added option for placing a vertical guideline on the Residuals\n",
    "            vs Leverage plot using the rule of thumb of h = 2p/n to denote\n",
    "            high leverage (high_leverage_threshold=True).\n",
    "        (5) Added two more ways to compute the Cook's Distance (D) threshold:\n",
    "            * 'baseR': D > 1 and D > 0.5 (default)\n",
    "            * 'convention': D > 4/n\n",
    "            * 'dof': D > 4 / (n - k - 1)\n",
    "        (6) Fixed class name to conform to Pascal casing convention\n",
    "        (7) Fixed Residuals vs Leverage legend to work with loc='best'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 results: Type[statsmodels.regression.linear_model.RegressionResultsWrapper]) -> None:\n",
    "        \"\"\"\n",
    "        For a linear regression model, generates following diagnostic plots:\n",
    "\n",
    "        a. residual\n",
    "        b. qq\n",
    "        c. scale location and\n",
    "        d. leverage\n",
    "\n",
    "        and a table\n",
    "\n",
    "        e. vif\n",
    "\n",
    "        Args:\n",
    "            results (Type[statsmodels.regression.linear_model.RegressionResultsWrapper]):\n",
    "                must be instance of statsmodels.regression.linear_model object\n",
    "\n",
    "        Raises:\n",
    "            TypeError: if instance does not belong to above object\n",
    "\n",
    "        Example:\n",
    "        >>> import numpy as np\n",
    "        >>> import pandas as pd\n",
    "        >>> import statsmodels.formula.api as smf\n",
    "        >>> x = np.linspace(-np.pi, np.pi, 100)\n",
    "        >>> y = 3*x + 8 + np.random.normal(0,1, 100)\n",
    "        >>> df = pd.DataFrame({'x':x, 'y':y})\n",
    "        >>> res = smf.ols(formula= \"y ~ x\", data=df).fit()\n",
    "        >>> cls = Linear_Reg_Diagnostic(res)\n",
    "        >>> cls(plot_context=\"seaborn-v0_8\")\n",
    "\n",
    "        In case you do not need all plots you can also independently make an individual plot/table\n",
    "        in following ways\n",
    "\n",
    "        >>> cls = Linear_Reg_Diagnostic(res)\n",
    "        >>> cls.residual_plot()\n",
    "        >>> cls.qq_plot()\n",
    "        >>> cls.scale_location_plot()\n",
    "        >>> cls.leverage_plot()\n",
    "        >>> cls.vif_table()\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(results, statsmodels.regression.linear_model.RegressionResultsWrapper) is False:\n",
    "            raise TypeError(\"result must be instance of statsmodels.regression.linear_model.RegressionResultsWrapper object\")\n",
    "\n",
    "        self.results = maybe_unwrap_results(results)\n",
    "\n",
    "        self.y_true = self.results.model.endog\n",
    "        self.y_predict = self.results.fittedvalues\n",
    "        self.xvar = self.results.model.exog\n",
    "        self.xvar_names = self.results.model.exog_names\n",
    "\n",
    "        self.residual = np.array(self.results.resid)\n",
    "        influence = self.results.get_influence()\n",
    "        self.residual_norm = influence.resid_studentized_internal\n",
    "        self.leverage = influence.hat_matrix_diag\n",
    "        self.cooks_distance = influence.cooks_distance[0]\n",
    "        self.nparams = len(self.results.params)\n",
    "        self.nresids = len(self.residual_norm)\n",
    "\n",
    "    def __call__(self, plot_context='seaborn-v0_8', **kwargs):\n",
    "        # print(plt.style.available)\n",
    "        # GH#9157\n",
    "        if plot_context not in plt.style.available:\n",
    "            plot_context = 'default'\n",
    "        with plt.style.context(plot_context):\n",
    "            fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n",
    "            self.residual_plot(ax=ax[0,0])\n",
    "            self.qq_plot(ax=ax[0,1])\n",
    "            self.scale_location_plot(ax=ax[1,0])\n",
    "            self.leverage_plot(\n",
    "                ax=ax[1,1],\n",
    "                high_leverage_threshold = kwargs.get('high_leverage_threshold'),\n",
    "                cooks_threshold = kwargs.get('cooks_threshold'))\n",
    "            plt.show()\n",
    "\n",
    "        return self.vif_table(), fig, ax,\n",
    "\n",
    "    def residual_plot(self, ax=None):\n",
    "        \"\"\"\n",
    "        Residual vs Fitted Plot\n",
    "\n",
    "        Graphical tool to identify non-linearity.\n",
    "        (Roughly) Horizontal red line is an indicator that the residual has a linear pattern\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        sns.residplot(\n",
    "            x=self.y_predict,\n",
    "            y=self.residual,\n",
    "            lowess=True,\n",
    "            scatter_kws={'alpha': 0.5},\n",
    "            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8},\n",
    "            ax=ax)\n",
    "\n",
    "        # annotations\n",
    "        residual_abs = np.abs(self.residual)\n",
    "        abs_resid = np.flip(np.argsort(residual_abs), 0)\n",
    "        abs_resid_top_3 = abs_resid[:3]\n",
    "        for i in abs_resid_top_3:\n",
    "            ax.annotate(\n",
    "                i,\n",
    "                xy=(self.y_predict[i], self.residual[i]),\n",
    "                color='C3')\n",
    "\n",
    "        ax.set_title('Residuals vs Fitted', fontweight=\"bold\")\n",
    "        ax.set_xlabel('Fitted values')\n",
    "        ax.set_ylabel('Residuals')\n",
    "        return ax\n",
    "\n",
    "    def qq_plot(self, ax=None):\n",
    "        \"\"\"\n",
    "        Standarized Residual vs Theoretical Quantile plot\n",
    "\n",
    "        Used to visually check if residuals are normally distributed.\n",
    "        Points spread along the diagonal line will suggest so.\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        QQ = ProbPlot(self.residual_norm)\n",
    "        fig = QQ.qqplot(line='45', alpha=0.5, lw=1, ax=ax)\n",
    "\n",
    "        # annotations\n",
    "        abs_norm_resid = np.flip(np.argsort(np.abs(self.residual_norm)), 0)\n",
    "        abs_norm_resid_top_3 = abs_norm_resid[:3]\n",
    "        for i, x, y in self.__qq_top_resid(QQ.theoretical_quantiles, abs_norm_resid_top_3):\n",
    "            ax.annotate(\n",
    "                i,\n",
    "                xy=(x, y),\n",
    "                ha='right',\n",
    "                color='C3')\n",
    "\n",
    "        ax.set_title('Normal Q-Q', fontweight=\"bold\")\n",
    "        ax.set_xlabel('Theoretical Quantiles')\n",
    "        ax.set_ylabel('Standardized Residuals')\n",
    "        return ax\n",
    "\n",
    "    def scale_location_plot(self, ax=None):\n",
    "        \"\"\"\n",
    "        Sqrt(Standarized Residual) vs Fitted values plot\n",
    "\n",
    "        Used to check homoscedasticity of the residuals.\n",
    "        Horizontal line will suggest so.\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        residual_norm_abs_sqrt = np.sqrt(np.abs(self.residual_norm))\n",
    "\n",
    "        ax.scatter(self.y_predict, residual_norm_abs_sqrt, alpha=0.5);\n",
    "        sns.regplot(\n",
    "            x=self.y_predict,\n",
    "            y=residual_norm_abs_sqrt,\n",
    "            scatter=False, ci=False,\n",
    "            lowess=True,\n",
    "            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8},\n",
    "            ax=ax)\n",
    "\n",
    "        # annotations\n",
    "        abs_sq_norm_resid = np.flip(np.argsort(residual_norm_abs_sqrt), 0)\n",
    "        abs_sq_norm_resid_top_3 = abs_sq_norm_resid[:3]\n",
    "        for i in abs_sq_norm_resid_top_3:\n",
    "            ax.annotate(\n",
    "                i,\n",
    "                xy=(self.y_predict[i], residual_norm_abs_sqrt[i]),\n",
    "                color='C3')\n",
    "\n",
    "        ax.set_title('Scale-Location', fontweight=\"bold\")\n",
    "        ax.set_xlabel('Fitted values')\n",
    "        ax.set_ylabel(r'$\\sqrt{|\\mathrm{Standardized\\ Residuals}|}$');\n",
    "        return ax\n",
    "\n",
    "    def leverage_plot(self, ax=None, high_leverage_threshold=False, cooks_threshold='baseR'):\n",
    "        \"\"\"\n",
    "        Residual vs Leverage plot\n",
    "\n",
    "        Points falling outside Cook's distance curves are considered observation that can sway the fit\n",
    "        aka are influential.\n",
    "        Good to have none outside the curves.\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        ax.scatter(\n",
    "            self.leverage,\n",
    "            self.residual_norm,\n",
    "            alpha=0.5);\n",
    "\n",
    "        sns.regplot(\n",
    "            x=self.leverage,\n",
    "            y=self.residual_norm,\n",
    "            scatter=False,\n",
    "            ci=False,\n",
    "            lowess=True,\n",
    "            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8},\n",
    "            ax=ax)\n",
    "\n",
    "        # annotations\n",
    "        leverage_top_3 = np.flip(np.argsort(self.cooks_distance), 0)[:3]\n",
    "        for i in leverage_top_3:\n",
    "            ax.annotate(\n",
    "                i,\n",
    "                xy=(self.leverage[i], self.residual_norm[i]),\n",
    "                color = 'C3')\n",
    "\n",
    "        factors = []\n",
    "        if cooks_threshold == 'baseR' or cooks_threshold is None:\n",
    "            factors = [1, 0.5]\n",
    "        elif cooks_threshold == 'convention':\n",
    "            factors = [4/self.nresids]\n",
    "        elif cooks_threshold == 'dof':\n",
    "            factors = [4/ (self.nresids - self.nparams)]\n",
    "        else:\n",
    "            raise ValueError(\"threshold_method must be one of the following: 'convention', 'dof', or 'baseR' (default)\")\n",
    "        for i, factor in enumerate(factors):\n",
    "            label = \"Cook's distance\" if i == 0 else None\n",
    "            xtemp, ytemp = self.__cooks_dist_line(factor)\n",
    "            ax.plot(xtemp, ytemp, label=label, lw=1.25, ls='--', color='red')\n",
    "            ax.plot(xtemp, np.negative(ytemp), lw=1.25, ls='--', color='red')\n",
    "\n",
    "        if high_leverage_threshold:\n",
    "            high_leverage = 2 * self.nparams / self.nresids\n",
    "            if max(self.leverage) > high_leverage:\n",
    "                ax.axvline(high_leverage, label='High leverage', ls='-.', color='purple', lw=1)\n",
    "\n",
    "        ax.axhline(0, ls='dotted', color='black', lw=1.25)\n",
    "        ax.set_xlim(0, max(self.leverage)+0.01)\n",
    "        ax.set_ylim(min(self.residual_norm)-0.1, max(self.residual_norm)+0.1)\n",
    "        ax.set_title('Residuals vs Leverage', fontweight=\"bold\")\n",
    "        ax.set_xlabel('Leverage')\n",
    "        ax.set_ylabel('Standardized Residuals')\n",
    "        plt.legend(loc='best')\n",
    "        return ax\n",
    "\n",
    "    def vif_table(self):\n",
    "        \"\"\"\n",
    "        VIF table\n",
    "\n",
    "        VIF, the variance inflation factor, is a measure of multicollinearity.\n",
    "        VIF > 5 for a variable indicates that it is highly collinear with the\n",
    "        other input variables.\n",
    "        \"\"\"\n",
    "        vif_df = pd.DataFrame()\n",
    "        vif_df[\"Features\"] = self.xvar_names\n",
    "        vif_df[\"VIF Factor\"] = [variance_inflation_factor(self.xvar, i) for i in range(self.xvar.shape[1])]\n",
    "\n",
    "        return (vif_df\n",
    "                .sort_values(\"VIF Factor\")\n",
    "                .round(2))\n",
    "\n",
    "\n",
    "    def __cooks_dist_line(self, factor):\n",
    "        \"\"\"\n",
    "        Helper function for plotting Cook's distance curves\n",
    "        \"\"\"\n",
    "        p = self.nparams\n",
    "        formula = lambda x: np.sqrt((factor * p * (1 - x)) / x)\n",
    "        x = np.linspace(0.001, max(self.leverage), 50)\n",
    "        y = formula(x)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def __qq_top_resid(self, quantiles, top_residual_indices):\n",
    "        \"\"\"\n",
    "        Helper generator function yielding the index and coordinates\n",
    "        \"\"\"\n",
    "        offset = 0\n",
    "        quant_index = 0\n",
    "        previous_is_negative = None\n",
    "        for resid_index in top_residual_indices:\n",
    "            y = self.residual_norm[resid_index]\n",
    "            is_negative = y < 0\n",
    "            if previous_is_negative == None or previous_is_negative == is_negative:\n",
    "                offset += 1\n",
    "            else:\n",
    "                quant_index -= offset\n",
    "            x = quantiles[quant_index] if is_negative else np.flip(quantiles, 0)[quant_index]\n",
    "            quant_index += 1\n",
    "            previous_is_negative = is_negative\n",
    "            yield resid_index, x, y\n",
    "\n",
    "def show_missing(df):\n",
    "    \"\"\"Return a Pandas dataframe describing the contents of a source dataframe including missing values.\"\"\"\n",
    "    pd.options.mode.use_inf_as_na = True   #to see inf values as NaN\n",
    "    variables = []\n",
    "    dtypes = []\n",
    "    count = []\n",
    "    missing = []\n",
    "    pc_missing = []\n",
    "\n",
    "    for item in df.columns:\n",
    "        variables.append(item)\n",
    "        dtypes.append(df[item].dtype)\n",
    "        count.append(len(df[item]))\n",
    "        missing.append(df[item].isna().sum())\n",
    "        pc_missing.append(round((df[item].isna().sum() / len(df[item])) * 100, 2))\n",
    "\n",
    "    output = pd.DataFrame({\n",
    "    'variable': variables, \n",
    "    'dtype': dtypes,\n",
    "    'count': count,\n",
    "    'missing': missing, \n",
    "    'pc_missing': pc_missing\n",
    "    })    \n",
    "\n",
    "    print(output.loc[output['missing'] > 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data <a class=\"anchor\" id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "path='' #set path to data\n",
    "\n",
    "#Read in data\n",
    "stats_file = os.path.join(path + 'data.csv')\n",
    "df = pd.read_csv(stats_file,sep=',', decimal=\".\", encoding='utf-8')\n",
    "df = df.replace(' ', np.nan)# set empty values to NaN, cause they missing\n",
    "df['Kind1_CBCL.Tsum'] = pd.to_numeric(df['Kind1_CBCL.Tsum'],errors='coerce')#NaN ingore \n",
    "final=df \n",
    "final = final.replace(['EG','KG'],[1.0,0.0]) #EG=1 KG=0\n",
    "\n",
    "#Split into F3 / F4 disorders\n",
    "#F4 = final[(final['Patient_PrimDiagnoseT1.ICD10'] == 'F41.1') | (final['Patient_PrimDiagnoseT1.ICD10'] == 'F43.21') | (final['Patient_PrimDiagnoseT1.ICD10'] == 'F45.1') | (final['Patient_PrimDiagnoseT1.ICD10'] == 'F43.10') | (final['Patient_PrimDiagnoseT1.ICD10'] == 'F41.0')| (final['Patient_PrimDiagnoseT1.ICD10'] == 'F40.218')| (final['Patient_PrimDiagnoseT1.ICD10'] == 'F43.23') | (final['Patient_PrimDiagnoseT1.ICD10'].isna())]\n",
    "#F3 = final[(final['Patient_PrimDiagnoseT1.ICD10'] == 'F34.1') | (final['Patient_PrimDiagnoseT1.ICD10'] == 'F33.1') | (final['Patient_PrimDiagnoseT1.ICD10'] == 'F32.0') | (final['Patient_PrimDiagnoseT1.ICD10'] == 'F33.41') | (final['Patient_PrimDiagnoseT1.ICD10'] == 'F32.4')| (final['Patient_PrimDiagnoseT1.ICD10'] == 'F34.1')| (final['Patient_PrimDiagnoseT1.ICD10'].isna())]\n",
    "\n",
    "\n",
    "#Take randomised one sibling out of the sample\n",
    "#import random\n",
    "#df_Gesch= df[df[\"Geschwisterkind\"]==\"ja\"]\n",
    "#df_ohne = df[df[\"Geschwisterkind\"]==\"nein\"]\n",
    "\n",
    "#91034 #91036 only one number exist as the other sibling didn´t made it trough quality control\n",
    "#df_ohne = pd.concat([df_ohne, df[df[\"famID\"] == 91034]])\n",
    "#df_ohne = pd.concat([df_ohne, df[df[\"famID\"] == 91036]])\n",
    "\n",
    "#ID = ('3060',\"3007\",\"1014\",\"91029\",\"91032\",\"93020\",\"91059\",\"93101\",\"3030\")\n",
    "#one_sib_total = pd.DataFrame()\n",
    "#random.seed(10)\n",
    "#for sibling in ID:   \n",
    "#    test = df_Gesch[df_Gesch[\"famID\"] == int(sibling)]\n",
    "#    one_sib = test.iloc[[random.randint(0,1)]]\n",
    "#    one_sib_total = pd.concat([one_sib_total,one_sib])\n",
    "#df_onesib = pd.concat([one_sib_total,df_ohne])  \n",
    "\n",
    "#z-transform the numeric data   \n",
    "    #df = full sample\n",
    "    #zdf  = full sample z-transformed\n",
    "    #zdf_onesib = randomised take one sibling z-transformed\n",
    "    #zdf_quest =  sample where all subjects with missing on questionnaire are removed z-transformed  \n",
    "zdf=df.copy()\n",
    "numeric_cols = zdf.select_dtypes(include=[np.number]).columns \n",
    "zdf[numeric_cols] = zdf[numeric_cols].apply(sc.zscore,nan_policy='omit')\n",
    "\n",
    "\n",
    "#zdf_onesib = df_onesib.copy()\n",
    "#numeric_cols = zdf_onesib.select_dtypes(include=[np.number]).columns \n",
    "#zdf_onesib[numeric_cols] = zdf_onesib[numeric_cols].apply(sc.zscore,nan_policy='omit')\n",
    "\n",
    "zdf_quest = zdf[zdf['Kind1_ESF.ES'].notna() & zdf['Kind1_FEEL_KJ.selbst_maladaptiv_gesamt'].notna() & zdf['Kind1_EFB.WS'].notna() & zdf[\"Kind1_FEEL_KJ.fremd_adaptiv_gesamt\"]]\n",
    "df_quest = df[df['Kind1_ESF.ES'].notna() & df['Kind1_FEEL_KJ.selbst_maladaptiv_gesamt'].notna() & df['Kind1_EFB.WS'].notna() & df[\"Kind1_FEEL_KJ.fremd_adaptiv_gesamt\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample description <a class=\"anchor\" id=\"sample\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #deactivate warnings \n",
    "#Sample description, Anzahl, Alter, Geschlecht, Geschwister, CBCL.Tsum\n",
    "print('Anzahl EG: ' + str(df[df['Gruppenzugehörigkeit']=='EG'].shape[0]) + '\\nMittleres Alter: ' + str(round(df['AlterJahreMonate'][df['Gruppenzugehörigkeit']=='EG'].mean(),3))+ ' (std: ' +str(round(df['AlterJahreMonate'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+ ')'+ '\\nGeschlecht: ' + str(df[df['Gruppenzugehörigkeit']=='EG'][df['Geschlecht']=='weiblich'].shape[0])+' weiblich und '+ str(df[df['Gruppenzugehörigkeit']=='EG'][df['Geschlecht']=='männlich'].shape[0]) + ' männlich'+ ' \\nGeschwister: ' + str(df[df['Geschwisterkind']=='ja'][df['Gruppenzugehörigkeit']=='EG'].shape[0])+ ' (4 Geschwisterpaare 2*4) \\nCBCL T-Wert Gesamtskala: ' + str(round(df['Kind1_CBCL.Tsum'][df['Gruppenzugehörigkeit']=='EG'].mean(skipna=True),3))+ ' (std: ' +str(round(df['Kind1_CBCL.Tsum'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+')'+ '\\nSES Gesamtskala: ' + str(round(df['SES_2'][df['Gruppenzugehörigkeit']=='EG'].mean(),3))+ ' (std: ' +str(round(df['SES_2'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+ ')')      \n",
    "print('\\n\\nAnzahl KG: ' + str(df[df['Gruppenzugehörigkeit']=='KG'].shape[0]) + '\\nMittleres Alter: ' + str(round(df['AlterJahreMonate'][df['Gruppenzugehörigkeit']=='KG'].mean(),3)) + ' (std: ' +str(round(df['AlterJahreMonate'][df['Gruppenzugehörigkeit']=='KG'].std(),3)) + ')' + '\\nGeschlecht: ' + str(df[df['Gruppenzugehörigkeit']=='KG'][df['Geschlecht']=='weiblich'].shape[0])+' weiblich und ' + str(df[df['Gruppenzugehörigkeit']=='KG'][df['Geschlecht']=='männlich'].shape[0]) + ' männlich' + '\\nGeschwister: ' + str(df[df['Geschwisterkind']=='ja'][df['Gruppenzugehörigkeit']=='KG'].shape[0]) + ' (5 Geschwisterpaare 2*5)' '\\nCBCL T-Wert Gesamtskala: ' + str(round(df['Kind1_CBCL.Tsum'][df['Gruppenzugehörigkeit']=='KG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_CBCL.Tsum'][df['Gruppenzugehörigkeit']=='KG'].std(),3))+ '\\nSES Gesamtskala: ' + str(round(df['SES_2'][df['Gruppenzugehörigkeit']=='KG'].mean(),3))+ ' (std: ' +str(round(df['SES_2'][df['Gruppenzugehörigkeit']=='KG'].std(),3))+ ')\\n\\n')\n",
    "print(\"Tage zwischen MRT und Labor: \\nmedian: \" +str(df['daysbetweenmriandlabor'].median()) + ' \\nmin Tage: '+str(df['daysbetweenmriandlabor'].min()) + '\\nmax Tage: '+str(df['daysbetweenmriandlabor'].max()))\n",
    "print('\\nAnzahl erhobenenr Probanden Gießen: ' + str(df[df['Standort']=='Gießen'].shape[0]))\n",
    "print('Anzahl erhobenenr Probanden Dortmund: ' + str(df[df['Standort']=='Dortmund'].shape[0]))  \n",
    "\n",
    "#check which dataframe \n",
    "Zwischenspeicher = df.copy()\n",
    "df = df_quest\n",
    "print('\\nEG = ' +  str(df[df['Gruppenzugehörigkeit']=='EG'].shape[0]), ' M(std):')\n",
    "print('Kind1_FEEL_KJ.fremd_maladaptiv_gesamt: ' + str(round(df['Kind1_FEEL_KJ.fremd_maladaptiv_gesamt'][df['Gruppenzugehörigkeit']=='EG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_FEEL_KJ.fremd_maladaptiv_gesamt'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+ ')')\n",
    "print('Kind1_FEEL_KJ.fremd_adaptiv_gesamt: ' + str(round(df['Kind1_FEEL_KJ.fremd_adaptiv_gesamt'][df['Gruppenzugehörigkeit']=='EG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_FEEL_KJ.fremd_adaptiv_gesamt'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+ ')')\n",
    "print('Kind1_FEEL_KJ.selbst_maladaptiv_gesamt: ' + str(round(df['Kind1_FEEL_KJ.selbst_maladaptiv_gesamt'][df['Gruppenzugehörigkeit']=='EG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_FEEL_KJ.selbst_maladaptiv_gesamt'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+ ')')\n",
    "print('Kind1_FEEL_KJ.selbst_adaptiv_gesamt: ' + str(round(df['Kind1_FEEL_KJ.selbst_adaptiv_gesamt'][df['Gruppenzugehörigkeit']=='EG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_FEEL_KJ.selbst_adaptiv_gesamt'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+ ')')\n",
    "print('Kind1_EFB.GW: ' + str(round(df['Kind1_EFB.GW'][df['Gruppenzugehörigkeit']=='EG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_EFB.GW'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+ ')')\n",
    "print('Kind1_EFB.NS: ' + str(round(df['Kind1_EFB.NS'][df['Gruppenzugehörigkeit']=='EG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_EFB.NS'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+ ')')\n",
    "print('Kind1_EFB.UR: ' + str(round(df['Kind1_EFB.UR'][df['Gruppenzugehörigkeit']=='EG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_EFB.UR'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+ ')')\n",
    "print('Kind1_EFB.WS: ' + str(round(df['Kind1_EFB.WS'][df['Gruppenzugehörigkeit']=='EG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_EFB.WS'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+ ')')\n",
    "print('Kind1_ESF.ES: ' + str(round(df['Kind1_ESF.ES'][df['Gruppenzugehörigkeit']=='EG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_ESF.ES'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+ ')')\n",
    "print('Kind1_ESF.RR: ' + str(round(df['Kind1_ESF.RR'][df['Gruppenzugehörigkeit']=='EG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_ESF.RR'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+ ')')\n",
    "print('Kind1_ESF.SU: ' + str(round(df['Kind1_ESF.SU'][df['Gruppenzugehörigkeit']=='EG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_ESF.SU'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+ ')')\n",
    "print('Kind1_ESF.PS: ' + str(round(df['Kind1_ESF.PS'][df['Gruppenzugehörigkeit']=='EG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_ESF.PS'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+ ')')\n",
    "print('AlterJahreMonate: ' + str(round(df['AlterJahreMonate'][df['Gruppenzugehörigkeit']=='EG'].mean(),3))+ ' (std: ' +str(round(df['AlterJahreMonate'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+ ')')\n",
    "print('SES_2: ' + str(round(df['SES_2'][df['Gruppenzugehörigkeit']=='EG'].mean(),3))+ ' (std: ' +str(round(df['SES_2'][df['Gruppenzugehörigkeit']=='EG'].std(),3))+ ')')\n",
    "print('Geschlecht: ' + str(df[df['Gruppenzugehörigkeit']=='EG'][df['Geschlecht']=='weiblich'].shape[0])+' weiblich und '+ str(df[df['Gruppenzugehörigkeit']=='EG'][df['Geschlecht']=='männlich'].shape[0]) + ' männlich')\n",
    "      \n",
    "print('\\nKG = ' +  str(df[df['Gruppenzugehörigkeit']=='KG'].shape[0]), ' M(std):')\n",
    "print('Kind1_FEEL_KJ.fremd_maladaptiv_gesamt: ' + str(round(df['Kind1_FEEL_KJ.fremd_maladaptiv_gesamt'][df['Gruppenzugehörigkeit']=='KG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_FEEL_KJ.fremd_maladaptiv_gesamt'][df['Gruppenzugehörigkeit']=='KG'].std(),3))+ ')')\n",
    "print('Kind1_FEEL_KJ.fremd_adaptiv_gesamt: ' + str(round(df['Kind1_FEEL_KJ.fremd_adaptiv_gesamt'][df['Gruppenzugehörigkeit']=='KG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_FEEL_KJ.fremd_adaptiv_gesamt'][df['Gruppenzugehörigkeit']=='KG'].std(),3))+ ')')\n",
    "print('Kind1_FEEL_KJ.selbst_maladaptiv_gesamt: ' + str(round(df['Kind1_FEEL_KJ.selbst_maladaptiv_gesamt'][df['Gruppenzugehörigkeit']=='KG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_FEEL_KJ.selbst_maladaptiv_gesamt'][df['Gruppenzugehörigkeit']=='KG'].std(),3))+ ')')\n",
    "print('Kind1_FEEL_KJ.selbst_adaptiv_gesamt: ' + str(round(df['Kind1_FEEL_KJ.selbst_adaptiv_gesamt'][df['Gruppenzugehörigkeit']=='KG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_FEEL_KJ.selbst_adaptiv_gesamt'][df['Gruppenzugehörigkeit']=='KG'].std(),3))+ ')')\n",
    "print('Kind1_EFB.GW: ' + str(round(df['Kind1_EFB.GW'][df['Gruppenzugehörigkeit']=='KG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_EFB.GW'][df['Gruppenzugehörigkeit']=='KG'].std(),3))+ ')')\n",
    "print('Kind1_EFB.NS: ' + str(round(df['Kind1_EFB.NS'][df['Gruppenzugehörigkeit']=='KG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_EFB.NS'][df['Gruppenzugehörigkeit']=='KG'].std(),3))+ ')')\n",
    "print('Kind1_EFB.UR: ' + str(round(df['Kind1_EFB.UR'][df['Gruppenzugehörigkeit']=='KG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_EFB.UR'][df['Gruppenzugehörigkeit']=='KG'].std(),3))+ ')')\n",
    "print('Kind1_EFB.WS: ' + str(round(df['Kind1_EFB.WS'][df['Gruppenzugehörigkeit']=='KG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_EFB.WS'][df['Gruppenzugehörigkeit']=='KG'].std(),3))+ ')')\n",
    "print('Kind1_ESF.ES: ' + str(round(df['Kind1_ESF.ES'][df['Gruppenzugehörigkeit']=='KG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_ESF.ES'][df['Gruppenzugehörigkeit']=='KG'].std(),3))+ ')')\n",
    "print('Kind1_ESF.RR: ' + str(round(df['Kind1_ESF.RR'][df['Gruppenzugehörigkeit']=='KG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_ESF.RR'][df['Gruppenzugehörigkeit']=='KG'].std(),3))+ ')')\n",
    "print('Kind1_ESF.SU: ' + str(round(df['Kind1_ESF.SU'][df['Gruppenzugehörigkeit']=='KG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_ESF.SU'][df['Gruppenzugehörigkeit']=='KG'].std(),3))+ ')')\n",
    "print('Kind1_ESF.PS: ' + str(round(df['Kind1_ESF.PS'][df['Gruppenzugehörigkeit']=='KG'].mean(),3))+ ' (std: ' +str(round(df['Kind1_ESF.PS'][df['Gruppenzugehörigkeit']=='KG'].std(),3))+ ')')\n",
    "print('AlterJahreMonate: ' + str(round(df['AlterJahreMonate'][df['Gruppenzugehörigkeit']=='KG'].mean(),3))+ ' (std: ' +str(round(df['AlterJahreMonate'][df['Gruppenzugehörigkeit']=='KG'].std(),3))+ ')')\n",
    "print('SES_2: ' + str(round(df['SES_2'][df['Gruppenzugehörigkeit']=='KG'].mean(),3))+ ' (std: ' +str(round(df['SES_2'][df['Gruppenzugehörigkeit']=='KG'].std(),3))+ ')')\n",
    "print('Geschlecht: ' + str(df[df['Gruppenzugehörigkeit']=='KG'][df['Geschlecht']=='weiblich'].shape[0])+' weiblich und '+ str(df[df['Gruppenzugehörigkeit']=='KG'][df['Geschlecht']=='männlich'].shape[0]) + ' männlich')\n",
    "\n",
    "print('\\nFull Sample = ' +  str(df.shape[0]), ' M(std):')\n",
    "print('Kind1_FEEL_KJ.fremd_maladaptiv_gesamt: ' + str(round(df['Kind1_FEEL_KJ.fremd_maladaptiv_gesamt'].mean(),3))+ ' (std: ' +str(round(df['Kind1_FEEL_KJ.fremd_maladaptiv_gesamt'].std(),3))+ ')')\n",
    "print('Kind1_FEEL_KJ.fremd_adaptiv_gesamt: ' + str(round(df['Kind1_FEEL_KJ.fremd_adaptiv_gesamt'].mean(),3))+ ' (std: ' +str(round(df['Kind1_FEEL_KJ.fremd_adaptiv_gesamt'].std(),3))+ ')')\n",
    "print('Kind1_FEEL_KJ.selbst_maladaptiv_gesamt: ' + str(round(df['Kind1_FEEL_KJ.selbst_maladaptiv_gesamt'].mean(),3))+ ' (std: ' +str(round(df['Kind1_FEEL_KJ.selbst_maladaptiv_gesamt'].std(),3))+ ')')\n",
    "print('Kind1_FEEL_KJ.selbst_adaptiv_gesamt: ' + str(round(df['Kind1_FEEL_KJ.selbst_adaptiv_gesamt'].mean(),3))+ ' (std: ' +str(round(df['Kind1_FEEL_KJ.selbst_adaptiv_gesamt'].std(),3))+ ')')\n",
    "print('Kind1_EFB.GW: ' + str(round(df['Kind1_EFB.GW'].mean(),3))+ ' (std: ' +str(round(df['Kind1_EFB.GW'].std(),3))+ ')')\n",
    "print('Kind1_EFB.NS: ' + str(round(df['Kind1_EFB.NS'].mean(),3))+ ' (std: ' +str(round(df['Kind1_EFB.NS'].std(),3))+ ')')\n",
    "print('Kind1_EFB.UR: ' + str(round(df['Kind1_EFB.UR'].mean(),3))+ ' (std: ' +str(round(df['Kind1_EFB.UR'].std(),3))+ ')')\n",
    "print('Kind1_EFB.WS: ' + str(round(df['Kind1_EFB.WS'].mean(),3))+ ' (std: ' +str(round(df['Kind1_EFB.WS'].std(),3))+ ')')\n",
    "print('Kind1_ESF.ES: ' + str(round(df['Kind1_ESF.ES'].mean(),3))+ ' (std: ' +str(round(df['Kind1_ESF.ES'].std(),3))+ ')')\n",
    "print('Kind1_ESF.RR: ' + str(round(df['Kind1_ESF.RR'].mean(),3))+ ' (std: ' +str(round(df['Kind1_ESF.RR'].std(),3))+ ')')\n",
    "print('Kind1_ESF.SU: ' + str(round(df['Kind1_ESF.SU'].mean(),3))+ ' (std: ' +str(round(df['Kind1_ESF.SU'].std(),3))+ ')')\n",
    "print('Kind1_ESF.PS: ' + str(round(df['Kind1_ESF.PS'].mean(),3))+ ' (std: ' +str(round(df['Kind1_ESF.PS'].std(),3))+ ')')\n",
    "print('AlterJahreMonate: ' + str(round(df['AlterJahreMonate'].mean(),3))+ ' (std: ' +str(round(df['AlterJahreMonate'].std(),3))+ ')')\n",
    "print('SES_2: ' + str(round(df['SES_2'].mean(),3))+ ' (std: ' +str(round(df['SES_2'].std(),3))+ ')')\n",
    "print('Kind1_CBCL.Tsum: ' + str(round(df['Kind1_CBCL.Tsum'].mean(),3))+ ' (std: ' +str(round(df['Kind1_CBCL.Tsum'].std(),3))+ ')')\n",
    "print('Geschlecht: ' + str(df[df['Geschlecht']=='weiblich'].shape[0])+' weiblich und '+ str(df[df['Geschlecht']=='männlich'].shape[0]) + ' männlich')\n",
    "print('Gruppe: ' + str(df[df['Gruppenzugehörigkeit']=='EG'].shape[0])+' EG und '+ str(df[df['Gruppenzugehörigkeit']=='KG'].shape[0]) + ' KG')\n",
    "\n",
    "#print('Geschlecht: ' + str(df[df['Geschlecht']=='weiblich'][df['Gruppenzugehörigkeit']==\"KG\"].shape[0]))\n",
    "#print('Geschlecht: ' + str(df[df['Geschlecht']=='männlich'][df['Gruppenzugehörigkeit']==\"KG\"].shape[0]))\n",
    "\n",
    "print('\\nAnzahl erhobener Probanden Gießen: ' + str(df[df['Standort']=='Gießen'].shape[0]))\n",
    "print('Anzahl erhobener Probanden Dortmund: ' + str(df[df['Standort']=='Dortmund'].shape[0]))  \n",
    "df = Zwischenspeicher.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Education and age <a class=\"anchor\" id=\"eduage\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#pie chart function, % and total sum are shown\n",
    "aseg_all = df\n",
    "def make_autopct(values):\n",
    "    def my_autopct(pct):\n",
    "        total = sum(values)\n",
    "        val = int(round(pct*total/100.0))\n",
    "        return '{p:.2f}% ({v:d})'.format(p=pct,v=val)#{p:.2f}% if %is needed\n",
    "    return my_autopct\n",
    "\n",
    "#subplot \n",
    "fig1, (ax1, ax2) = plt.subplots(1,2, figsize=(18,10)) # 1 row, 2 columns\n",
    "fig1.tight_layout(w_pad=12)\n",
    "\n",
    "#plot histogram with fitted norm curve\n",
    "mu, std = norm.fit(aseg_all['AlterJahreMonate'])\n",
    "ax2.hist(aseg_all['AlterJahreMonate'], bins=15, alpha=0.6, edgecolor='black',density=True)\n",
    "ax2.set_title('Age histogram of children')\n",
    "ax2.set_xlabel('Age')\n",
    "ax2.set_ylabel('density')\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "ax2.plot(x, p, 'k', linewidth=2)\n",
    "\n",
    "#plot pie chart\n",
    "af = aseg_all.groupby(['Kind1_schule'])['Kind1_schule'].count().reset_index(name='count')\n",
    "ax1.pie(af['count'], labels = af['Kind1_schule'],\n",
    "       autopct=make_autopct(af['count']),\n",
    "       wedgeprops = {\"edgecolor\" : \"black\", 'linewidth': 0.5, 'antialiased': True})\n",
    "ax1.set_title(\"Education\")\n",
    "plt.rc ('font', size = 25) \n",
    "\n",
    "dfe = aseg_all[aseg_all['Gruppenzugehörigkeit'] == 'EG']#only EG Group\n",
    "dfk = aseg_all[aseg_all['Gruppenzugehörigkeit'] == 'KG']#only KG Group\n",
    "\n",
    "#count and %, children schoolform for EG and KG \n",
    "counts = dfe['Kind1_schule'].value_counts()\n",
    "percs = dfe['Kind1_schule'].value_counts(normalize=True).mul(100).round(2)\n",
    "print('experimental group\\n' + str(pd.concat([counts,percs], axis=1, keys=['count', 'percentage'])))\n",
    "\n",
    "counts = dfk['Kind1_schule'].value_counts()\n",
    "percs = dfk['Kind1_schule'].value_counts(normalize=True).mul(100).round(2)\n",
    "print('\\n\\ncontrol group\\n' + str(pd.concat([counts,percs], axis=1, keys=['count', 'percentage'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Daten (N je Stufe): at-risk und control\n",
    "levels = [\"Gymnasium\",\"Grundschule\",\"Gesamtschule\",\"Realschule\",\"Kindergarten\"]\n",
    "at_risk = np.array([10, 9, 7, 1, 1])\n",
    "control = np.array([17,12, 4, 3, 0])\n",
    "\n",
    "# Einzelfalldaten erzeugen (Long-Format)\n",
    "edu, group = [], []\n",
    "for i, lvl in enumerate(levels):\n",
    "    edu   += [lvl]*at_risk[i];  group += [\"at_risk\"]*at_risk[i]\n",
    "    edu   += [lvl]*control[i];  group += [\"control\"]*control[i]\n",
    "\n",
    "df = pd.DataFrame({\"education\": edu, \"group\": group})\n",
    "\n",
    "# Chi-Quadrat-Statistik auf den Originaldaten (nur zur Information)\n",
    "tab = pd.crosstab(df[\"education\"], df[\"group\"])\n",
    "chi2_obs = chi2_contingency(tab, correction=False)[0]\n",
    "\n",
    "# Nicht-konditionaler Permutationstest\n",
    "rng = np.random.default_rng(123)\n",
    "B = 100_000  # Anzahl Permutationen (für schnelle Tests z.B. 20_000)\n",
    "stats = np.empty(B)\n",
    "\n",
    "g = df[\"group\"].to_numpy()\n",
    "for b in range(B):\n",
    "    perm = rng.permutation(g)\n",
    "    tab_b = pd.crosstab(df[\"education\"], perm)\n",
    "    stats[b] = chi2_contingency(tab_b, correction=False)[0]\n",
    "\n",
    "p_perm = (np.sum(stats >= chi2_obs) + 1) / (B + 1)\n",
    "chi2_obs, p_perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parental disorder <a class=\"anchor\" id=\"dis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dfe['Patient_Alter_or_moth'] = pd.to_numeric(dfe['Patient_Alter_or_moth'],errors='coerce')#NaN ingore \n",
    "dfk['Patient_Alter_or_moth'] = pd.to_numeric(dfk['Patient_Alter_or_moth'],errors='coerce')#NaN ingore \n",
    "print('ALter: ' + str(round(dfe['Patient_Alter_or_moth'][dfe['Gruppenzugehörigkeit']=='EG'].mean(),2)) + ' STD: '+  str(round(dfe['Patient_Alter_or_moth'][dfe['Gruppenzugehörigkeit']=='EG'].std(),2)))\n",
    "print(str(dfe['Patient_geschlecht'][dfe['Gruppenzugehörigkeit']=='EG'].value_counts()))\n",
    "\n",
    "print('\\nMütter der KG Alter: ' + str(round(dfk['Patient_Alter_or_moth'][dfk['Gruppenzugehörigkeit']=='KG'].mean(),2)) + ' STD: '+  str(round(dfk['Patient_Alter_or_moth'][dfk['Gruppenzugehörigkeit']=='KG'].std(),2)))\n",
    "\n",
    "#plot pie chart with parent illnes\n",
    "af = df.groupby(['Patient_PrimDiagnoseT1.ICD10'])['Patient_PrimDiagnoseT1.ICD10'].count().reset_index(name='count')\n",
    "\n",
    "fig, ax = plt.subplots(figsize =(24, 12))\n",
    "fig.set_facecolor('white')\n",
    "colors = ['red','green','green','green','orange','orange','purple','beige','beige','violet','violet','yellow','yellow','yellow','turquoise']\n",
    "explode = (0.1, 0.1, 0.05, 0.05, 0.1, 0.05, 0.02, 0.02, 0.05, 0.05, 0.05, 0.25) \n",
    "\n",
    "wedges, texts, autotexts = ax.pie(af['count'], labels = af['Patient_PrimDiagnoseT1.ICD10'],\n",
    "                                  colors =colors,\n",
    "                                  autopct=make_autopct(af['count']),\n",
    "                                  explode =explode,\n",
    "                                  wedgeprops = {\"edgecolor\" : \"black\", 'linewidth': 0.5, 'antialiased': True})\n",
    "\n",
    "ax.set_title(\"Parental disorders\")\n",
    "plt.rc ('font', size = 15) \n",
    "#ax.legend(wedges, af['Patient_PrimDiagnoseT1.ICD10'],\n",
    "          #title ='Parental disorders',\n",
    "          #loc =\"center left\",\n",
    "          #bbox_to_anchor =(1.25, 0, 0.5, 1))\n",
    "   \n",
    "plt.show() \n",
    "\n",
    "print('Mood [affective] disorders & Neurotic, stress-related and somatoform disorders')\n",
    "print('Delusional disorder vs. Depressive Episode vs. Recurrent depressive disorder vs. Dysthymia vs. Phobic anxiety disorders vs. Other anxiety disorders vs. Reaction to severe stress, and adjustment disorders vs Somatoform disorders')\n",
    "\n",
    "#plot how many parental diagnosis exist and which one\n",
    "#fig, axes = plt.subplots(5, 1, figsize=(18, 10))\n",
    "#fig.set_figwidth(24)\n",
    "#fig.set_figheight(18)\n",
    "#fig.tight_layout(w_pad=12)\n",
    "\n",
    "#sns.countplot(y=df['anzahl_diagT1'],ax=axes[0])\n",
    "#sns.countplot(y=df['Patient_sek1DiagnoseT1'],alpha=1,ax=axes[1])\n",
    "#sns.countplot(y=df['Patient_sek2DiagnoseT1'],alpha=1,ax=axes[2])\n",
    "#sns.countplot(y=df['Patient_sek3DiagnoseT1'],alpha=1,ax=axes[3])\n",
    "#sns.countplot(y=df['Patient_sek4DiagnoseT1'],alpha=1,ax=axes[4])\n",
    "\n",
    "print('\\n\\nNumber of diagnoses:\\n' + str(df['anzahl_diagT1'].value_counts()))\n",
    "print('\\n\\nSecond diagnosis:\\n' + str(df['Patient_sek1DiagnoseT1'].value_counts()))\n",
    "print('\\n\\nThird diagnosis:\\n' + str(df['Patient_sek2DiagnoseT1'].value_counts()))\n",
    "print('\\n\\nFourth diagnosis:\\n' + str(df['Patient_sek3DiagnoseT1'].value_counts()))\n",
    "print('\\n\\nFifth diagnosis:\\n' + str(df['Patient_sek4DiagnoseT1'].value_counts()))\n",
    "\n",
    "print('\\n\\nmostly F3 and F4 disorders - depressive episode, Phobic anxiety disorders, anxiety disorders')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample Comparison <a class=\"anchor\" id=\"samplecom\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "dfttest= df_quest.filter([\"Geschwisterkind\",'lhilf_avFA_Avg','lhslf1_aFA_Avg', 'lhslf2_aFA_Avg', 'lhslf3_aFA_Avg','lhuf_avgFA_Avg', 'rhilf_avFA_Avg','rhslf1_aFA_Avg', 'rhslf2_aFA_Avg', 'rhslf3_aFA_Avg', 'rhuf_avgFA_Avg','ccgenu_aFA_Avg','rhcbd_avFA_Avg', 'rhcbv_avFA_Avg','lhcbd_avFA_Avg', 'lhcbv_avFA_Avg','Gruppenzugehörigkeit', 'Geschlecht', 'AlterJahreMonate', 'Standort','Kind1_CBCL.Tsum','Kind1_FEEL_KJ.selbst_adaptiv_gesamt','Kind1_FEEL_KJ.selbst_maladaptiv_gesamt','SES_2','Kind1_CBCL.TEXT','Kind1_CBCL.TINT','Kind1_FEEL_KJ.fremd_adaptiv_gesamt','Kind1_FEEL_KJ.fremd_maladaptiv_gesamt', 'Kind1_EFB.NS', 'Kind1_EFB.UR','Kind1_EFB.WS','Kind1_EFB.GW','Kind1_ESF.ES','Kind1_ESF.RR','Kind1_ESF.SU','Kind1_ESF.PS'])\n",
    "dfttest = dfttest.replace(['männlich','weiblich','Dortmund','Gießen','ja', 'nein','EG','KG'],[1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0])\n",
    "\n",
    "cols  = ['Kind1_FEEL_KJ.fremd_adaptiv_gesamt','Kind1_FEEL_KJ.fremd_maladaptiv_gesamt','Kind1_FEEL_KJ.selbst_adaptiv_gesamt','Kind1_FEEL_KJ.selbst_maladaptiv_gesamt','Kind1_CBCL.TEXT','Kind1_CBCL.TINT','Kind1_CBCL.Tsum', 'Kind1_EFB.NS', 'Kind1_EFB.UR','Kind1_EFB.WS','Kind1_EFB.GW','Kind1_ESF.ES','Kind1_ESF.RR','Kind1_ESF.SU','Kind1_ESF.PS','SES_2','AlterJahreMonate']\n",
    "cond = dfttest['Gruppenzugehörigkeit'] == 0\n",
    "\n",
    "neg_outcome = dfttest.loc[cond, cols]\n",
    "pos_outcome = dfttest.loc[~cond, cols]\n",
    "\n",
    "# Welch ttest EG = 11 vs KG = 35\n",
    "t, p = sc.ttest_ind(neg_outcome, pos_outcome, equal_var=False)\n",
    "for i, col in enumerate(cols):\n",
    "    print(f'\\t{col}: t = {t[i]:.5f}, with p-value = {p[i]:.5f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentist Statistic <a class=\"anchor\" id=\"frequentist\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BoxPlot <a class=\"anchor\" id=\"boxplot\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "FAhy = ('lhilf_avFA_Avg','lhslf1_aFA_Avg', 'lhslf2_aFA_Avg', 'lhslf3_aFA_Avg','lhuf_avgFA_Avg', 'rhilf_avFA_Avg','rhslf1_aFA_Avg', 'rhslf2_aFA_Avg', 'rhslf3_aFA_Avg', 'rhuf_avgFA_Avg','ccgenu_aFA_Avg','rhcbd_avFA_Avg', 'rhcbv_avFA_Avg','lhcbd_avFA_Avg', 'lhcbv_avFA_Avg')\n",
    "ADhy = ('lhilf_avAd_Avg','lhslf1_aAd_Avg', 'lhslf2_aAd_Avg','lhslf3_aAd_Avg', 'lhuf_avgAd_Avg', 'rhilf_avAd_Avg','rhslf1_aAd_Avg', 'rhslf2_aAd_Avg', 'rhslf3_aAd_Avg', 'rhuf_avgAd_Avg','ccgenu_aAd_Avg','rhcbd_avAd_Avg', 'rhcbv_avAd_Avg', 'lhcbd_avAd_Avg', 'lhcbv_avAd_Avg')\n",
    "MDhy = ('lhilf_avMD_Avg','lhslf1_aMD_Avg', 'lhslf2_aMD_Avg','lhslf3_aMD_Avg', 'lhuf_avgMD_Avg', 'rhilf_avMD_Avg', 'rhslf1_aMD_Avg', 'rhslf2_aMD_Avg', 'rhslf3_aMD_Avg', 'rhuf_avgMD_Avg','ccgenu_aMD_Avg','rhcbd_avMD_Avg', 'rhcbv_avMD_Avg', 'lhcbd_avMD_Avg', 'lhcbv_avMD_Avg')\n",
    "RDhy = ('lhilf_avRD_Avg','lhslf1_aRD_Avg', 'lhslf2_aRD_Avg','lhslf3_aRD_Avg', 'lhuf_avgRD_Avg', 'rhilf_avRD_Avg', 'rhslf1_aRD_Avg', 'rhslf2_aRD_Avg', 'rhslf3_aRD_Avg', 'rhuf_avgRD_Avg','ccgenu_aRD_Avg','rhcbd_avRD_Avg', 'rhcbv_avRD_Avg', 'lhcbd_avRD_Avg', 'lhcbv_avRD_Avg')\n",
    "\n",
    "fig1, axs = plt.subplots(nrows=3,ncols=5, figsize=(30,11)) # 2 row, 5 columns\n",
    "sns.set_theme()\n",
    "ResKS =[]\n",
    "for ax, response in zip(axs.ravel(),FAhy):\n",
    "    sns.boxplot(data=final, x='Gruppenzugehörigkeit', y=response, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KG vs. EG <a class=\"anchor\" id=\"vs\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "def format_ols_results(para):\n",
    "    dff = pd.DataFrame() \n",
    "    dff['Param'] = para.params\n",
    "    dff['tvalues'] = para.tvalues\n",
    "    dff['pvalues'] = para.pvalues\n",
    "    dff['rsquared'] = para.rsquared\n",
    "    dff['rsquared_adj'] = para.rsquared_adj\n",
    "    return dff\n",
    "\n",
    "def plotRoi(roi_cols,predictor,Atlas,VolSuf,A,height): \n",
    "    n_comparisons =  len(roi_cols) \n",
    "    alpha = 0.05\n",
    "    alpha_corr = 0.05 / n_comparisons\n",
    "    g = sns.catplot(x='pvalues', y='Areal', kind='bar', height=height, data=A)\n",
    "    g.set(xscale='log', xlim=(1e-3,2))\n",
    "    sns.set(rc={'figure.figsize':(30,24)})\n",
    "    print('Prädiktor '+ predictor+ ' auf '+'Kirterium mit den Covariaten ' + covariates+ ' und p_value_corr: ' + str(alpha_corr))\n",
    "\n",
    "    for ax in g.axes.flat:\n",
    "        ax.axvline(alpha, ls='--',c='tomato')\n",
    "        ax.axvline(alpha_corr, ls='--',c='darkred')\n",
    "        \n",
    "#dummycoding\n",
    "zdf = zdf.replace(['EG','KG'],[1.0,0.0])\n",
    "df = df.replace(['EG','KG'],[1.0,0.0])\n",
    "zdf_quest = zdf_quest.replace(['EG','KG'],[1.0,0.0])\n",
    "\n",
    "#rename for model\n",
    "zdf_quest = zdf_quest.rename(columns={\"Kind1_FEEL_KJ.fremd_adaptiv_gesamt\" : \"Kind1_FEEL_KJ_fremd_adaptiv_gesamt\",\n",
    "                                      \"Kind1_FEEL_KJ.fremd_maladaptiv_gesamt\" : \"Kind1_FEEL_KJ_fremd_maladaptiv_gesamt\",\n",
    "                                      \"Kind1_FEEL_KJ.selbst_adaptiv_gesamt\" : \"Kind1_FEEL_KJ_selbst_adaptiv_gesamt\",\n",
    "                                      \"Kind1_FEEL_KJ.selbst_maladaptiv_gesamt\" : \"Kind1_FEEL_KJ_selbst_maladaptiv_gesamt\",\n",
    "                                      \"Kind1_EFB.NS\" : \"Kind1_EFB_NS\",\n",
    "                                      \"Kind1_EFB.UR\" : \"Kind1_EFB_UR\",\n",
    "                                      \"Kind1_EFB.WS\" : \"Kind1_EFB_WS\",\n",
    "                                      \"Kind1_EFB.GW\" : \"Kind1_EFB_GW\",\n",
    "                                      \"Kind1_ESF.ES\" : \"Kind1_ESF_ES\",\n",
    "                                      \"Kind1_ESF.RR\" : \"Kind1_ESF_RR\",\n",
    "                                      \"Kind1_ESF.SU\" : \"Kind1_ESF_SU\",\n",
    "                                      \"Kind1_ESF.PS\" : \"Kind1_ESF_PS\"})\n",
    "\n",
    "#colors\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "   \n",
    "#ROIS\n",
    "FAhy = ('lhilf_avFA_Avg','lhslf1_aFA_Avg', 'lhslf2_aFA_Avg', 'lhslf3_aFA_Avg','lhuf_avgFA_Avg', 'rhilf_avFA_Avg','rhslf1_aFA_Avg', 'rhslf2_aFA_Avg', 'rhslf3_aFA_Avg', 'rhuf_avgFA_Avg','ccgenu_aFA_Avg', 'rhcbd_avFA_Avg', 'rhcbv_avFA_Avg','lhcbd_avFA_Avg', 'lhcbv_avFA_Avg')\n",
    "MDhy = ('lhilf_avMD_Avg','lhslf1_aMD_Avg', 'lhslf2_aMD_Avg','lhslf3_aMD_Avg', 'lhuf_avgMD_Avg', 'rhilf_avMD_Avg', 'rhslf1_aMD_Avg', 'rhslf2_aMD_Avg', 'rhslf3_aMD_Avg', 'rhuf_avgMD_Avg','ccgenu_aMD_Avg', 'rhcbd_avMD_Avg', 'rhcbv_avMD_Avg', 'lhcbd_avMD_Avg', 'lhcbv_avMD_Avg')\n",
    "\n",
    "ADhy = ('lhilf_avAd_Avg','lhslf1_aAd_Avg', 'lhslf2_aAd_Avg','lhslf3_aAd_Avg', 'lhuf_avgAd_Avg', 'rhilf_avAd_Avg','rhslf1_aAd_Avg', 'rhslf2_aAd_Avg', 'rhslf3_aAd_Avg', 'rhuf_avgAd_Avg','ccgenu_aAd_Avg','rhcbd_avAd_Avg', 'rhcbv_avAd_Avg', 'lhcbd_avAd_Avg', 'lhcbv_avAd_Avg')\n",
    "RDhy = ('lhilf_avRD_Avg','lhslf1_aRD_Avg', 'lhslf2_aRD_Avg','lhslf3_aRD_Avg', 'lhuf_avgRD_Avg', 'rhilf_avRD_Avg', 'rhslf1_aRD_Avg', 'rhslf2_aRD_Avg', 'rhslf3_aRD_Avg', 'rhuf_avgRD_Avg','ccgenu_aRD_Avg','rhcbd_avRD_Avg', 'rhcbv_avRD_Avg', 'lhcbd_avRD_Avg', 'lhcbv_avRD_Avg')\n",
    "\n",
    "#Hypothesis ROIS\n",
    "ILF = ('lhilf_avFA_Avg' , 'lhilf_avMD_Avg' , 'rhilf_avFA_Avg', 'rhilf_avMD_Avg')\n",
    "SLF = ( 'lhslf1_aFA_Avg', 'lhslf2_aFA_Avg', 'lhslf3_aFA_Avg', 'lhslf1_aMD_Avg', 'lhslf2_aMD_Avg','lhslf3_aMD_Avg','rhslf1_aFA_Avg', 'rhslf2_aFA_Avg', 'rhslf3_aFA_Avg', 'rhslf1_aMD_Avg', 'rhslf2_aMD_Avg', 'rhslf3_aMD_Avg')\n",
    "UF = ('lhuf_avgFA_Avg','lhuf_avgMD_Avg','rhuf_avgFA_Avg', 'rhuf_avgMD_Avg')\n",
    "CC = ('ccgenu_aFA_Avg', 'ccgenu_aMD_Avg')\n",
    "Cing = ('rhcbd_avFA_Avg', 'rhcbd_avMD_Avg', 'rhcbv_avFA_Avg', 'rhcbv_avMD_Avg','lhcbd_avFA_Avg', 'lhcbd_avMD_Avg', 'lhcbv_avFA_Avg', 'lhcbv_avMD_Avg')\n",
    "\n",
    "\n",
    "predictor = ('Gruppenzugehörigkeit')\n",
    "covariates = ('AlterJahreMonate  + C(Geschlecht,Treatment(reference=\"männlich\"))  + C(Standort,Treatment(reference=\"Dortmund\")) + SES_2 ') \n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "ols_df = pd.DataFrame()\n",
    "for response in SLF:   #change ILF,SLF,UF,CC,Cing\n",
    "    res = smf.ols(f\"{response} ~ {covariates} + {predictor}\", data=zdf).fit()\n",
    "    res_df = format_ols_results(res)\n",
    "    res_df['Areal'] = response\n",
    "    ols_df = pd.concat([res_df,ols_df], axis=0, join='outer')\n",
    "    \n",
    "    print('\\n\\n',color.BOLD, color.RED ,response, color.END, res.summary())\n",
    "A = ols_df.loc['Gruppenzugehörigkeit']\n",
    "plotRoi(Cing,predictor,'Tracula White matter','(FA)',A,5) #change vektor\n",
    "\n",
    "#Multiple Comparison correction Bonferroni Holm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print('\\n Bonferroni-Holm:', multipletests(A['pvalues'],method='holm', is_sorted=False, returnsorted=False))\n",
    "round(A,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "\n",
    "for response in Cing: #change ILF,SLF,UF,CC,Cing\n",
    "    res = smf.ols(f\"{response} ~ {covariates} + {predictor}\", data=zdf).fit()\n",
    "    print('\\n\\n' +\"\\033[1m\"+\"\\033[1;37;40m\" ,response,\"\\033[0m\")\n",
    "    #print(res.summary())\n",
    "    cls = LinearRegDiagnostic(res)\n",
    "    vif, fig, ax = cls()\n",
    "    #print(vif)\n",
    "    #dw= durbin_watson(res.resid)\n",
    "    #print(f\"\\n\\n Durbin-Watson: {dw}\")#1.5-2.5 good Unkorreliertheit der Residuen bzw. Fehler\n",
    "    \n",
    "    #print(OLSInfluence(res).summary_table(float_fmt='%6.3f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation parenting skill <a class=\"anchor\" id=\"corr\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ILF_ = df.filter(['lhilf_avFA_Avg' , 'lhilf_avMD_Avg' , 'rhilf_avFA_Avg', 'rhilf_avMD_Avg','Kind1_EFB.GW','Gruppenzugehörigkeit'])\n",
    "SLF_ = df.filter(['lhslf1_aFA_Avg', 'lhslf2_aFA_Avg', 'lhslf3_aFA_Avg', 'lhslf1_aMD_Avg', 'lhslf2_aMD_Avg','lhslf3_aMD_Avg','rhslf1_aFA_Avg', 'rhslf2_aFA_Avg', 'rhslf3_aFA_Avg', 'rhslf1_aMD_Avg', 'rhslf2_aMD_Avg', 'rhslf3_aMD_Avg','Kind1_EFB.GW','Gruppenzugehörigkeit'])\n",
    "UF_ = df.filter(['lhuf_avgFA_Avg','lhuf_avgMD_Avg','rhuf_avgFA_Avg', 'rhuf_avgMD_Avg','Kind1_EFB.GW','Gruppenzugehörigkeit'])\n",
    "CC_ = df.filter(['ccgenu_aFA_Avg', 'ccgenu_aMD_Avg','Kind1_EFB.GW','Gruppenzugehörigkeit'])\n",
    "Cing_ = df.filter(['rhcbd_avFA_Avg', 'rhcbd_avMD_Avg', 'rhcbv_avFA_Avg', 'rhcbv_avMD_Avg','lhcbd_avFA_Avg', 'lhcbd_avMD_Avg', 'lhcbv_avFA_Avg', 'lhcbv_avMD_Avg','Kind1_EFB.GW','Gruppenzugehörigkeit'])\n",
    "\n",
    "sns.set_theme(style=\"ticks\")\n",
    "sns.pairplot(SLF_, hue=\"Gruppenzugehörigkeit\") #change ILF_;SLF_;UF_;CC_;Cing_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "zdf_quest_without = zdf_quest.drop(index=34)\n",
    "\n",
    "predictor = ('Gruppenzugehörigkeit+Kind1_EFB_GW')\n",
    "covariates = ('SES_2 + AlterJahreMonate  + C(Geschlecht,Treatment(reference=\"männlich\"))  + C(Standort,Treatment(reference=\"Dortmund\"))') \n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "ols_df = pd.DataFrame()\n",
    "for response in Cing:   #change ILF,SLF,UF,CC,Cing\n",
    "    res = smf.ols(f\"{response} ~ {covariates} + {predictor}\", data=zdf_quest).fit()\n",
    "    res_df = format_ols_results(res)\n",
    "    res_df['Areal'] = response\n",
    "    ols_df = pd.concat([res_df,ols_df], axis=0, join='outer')\n",
    "    \n",
    "    print('\\n\\n',color.BOLD, color.RED ,response, color.END, res.summary())\n",
    "A = ols_df.loc['Kind1_EFB_GW']\n",
    "#plotRoi(Cing,predictor,'Tracula White matter','(FA)',A,5) #change vektor\n",
    "\n",
    "\n",
    "#Multiple Comparison correction Bonferroni Holm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print('\\n Bonferroni-Holm:', multipletests(A['pvalues'],method='holm', is_sorted=False, returnsorted=False))\n",
    "round(A,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "for response in SLF: #change ILF,SLF,UF,CC,Cing\n",
    "    res = smf.ols(f\"{response} ~ {covariates} + {predictor}\", data=zdf_quest_without).fit()\n",
    "    print('\\n\\n' +\"\\033[1m\"+\"\\033[1;37;40m\" ,response,\"\\033[0m\")\n",
    "    #print(res.summary())\n",
    "    cls = LinearRegDiagnostic(res)\n",
    "    vif, fig, ax = cls()\n",
    "    #print(vif)\n",
    "    #dw= durbin_watson(res.resid)\n",
    "    #print(f\"\\n\\n Durbin-Watson: {dw}\")#1.5-2.5 good Unkorreliertheit der Residuen bzw. Fehler\n",
    "    \n",
    "    #print(OLSInfluence(res).summary_table(float_fmt='%6.3f'))\n",
    "    \n",
    "#Looks like row 17 has much influence in the regression it corresponde to index 33 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation parenting stress <a class=\"anchor\" id=\"corrstress\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ILF_ = df.filter(['lhilf_avFA_Avg' , 'lhilf_avMD_Avg' , 'rhilf_avFA_Avg', 'rhilf_avMD_Avg','Kind1_ESF.ES','Kind1_ESF.RR','Kind1_ESF.SU','Kind1_ESF.PS','Gruppenzugehörigkeit'])\n",
    "SLF_ = df.filter(['lhslf1_aFA_Avg', 'lhslf2_aFA_Avg', 'lhslf3_aFA_Avg', 'lhslf1_aMD_Avg', 'lhslf2_aMD_Avg','lhslf3_aMD_Avg','rhslf1_aFA_Avg', 'rhslf2_aFA_Avg', 'rhslf3_aFA_Avg', 'rhslf1_aMD_Avg', 'rhslf2_aMD_Avg', 'rhslf3_aMD_Avg','Kind1_ESF.ES','Kind1_ESF.RR','Kind1_ESF.SU','Kind1_ESF.PS','Gruppenzugehörigkeit'])\n",
    "UF_ = df.filter(['lhuf_avgFA_Avg','lhuf_avgMD_Avg','rhuf_avgFA_Avg', 'rhuf_avgMD_Avg','Kind1_ESF.ES','Kind1_ESF.RR','Kind1_ESF.SU','Kind1_ESF.PS','Gruppenzugehörigkeit'])\n",
    "CC_ = df.filter(['ccgenu_aFA_Avg', 'ccgenu_aMD_Avg','Kind1_ESF.ES','Kind1_ESF.RR','Kind1_ESF.SU','Kind1_ESF.PS','Gruppenzugehörigkeit'])\n",
    "Cing_ = df.filter(['rhcbd_avFA_Avg', 'rhcbd_avMD_Avg', 'rhcbv_avFA_Avg', 'rhcbv_avMD_Avg','lhcbd_avFA_Avg', 'lhcbd_avMD_Avg', 'lhcbv_avFA_Avg', 'lhcbv_avMD_Avg','Kind1_ESF.ES','Kind1_ESF.RR','Kind1_ESF.SU','Kind1_ESF.PS','Gruppenzugehörigkeit'])\n",
    "\n",
    "sns.set_theme(style=\"ticks\")\n",
    "sns.pairplot(ILF_, hue=\"Gruppenzugehörigkeit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "##predictor \n",
    "\n",
    "#\"Gruppenzugehörigkeit+Kind1_ESF_ES\"\n",
    "#\"Gruppenzugehörigkeit+Kind1_ESF_RR\"\n",
    "#\"Gruppenzugehörigkeit+Kind1_ESF_SU\"\n",
    "#\"Gruppenzugehörigkeit+Kind1_ESF_PS\"\n",
    "\n",
    "covariates = ('SES_2 + AlterJahreMonate  + C(Geschlecht,Treatment(reference=\"männlich\"))  + C(Standort,Treatment(reference=\"Dortmund\"))')\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "ols_df_ES = pd.DataFrame()\n",
    "ols_df_RR = pd.DataFrame()\n",
    "ols_df_SU = pd.DataFrame()\n",
    "ols_df_PS = pd.DataFrame()\n",
    "for response in Cing:   #change ILF,SLF,UF,CC,Cing\n",
    "    res_ES = smf.ols(f\"{response} ~ {covariates} + Gruppenzugehörigkeit+Kind1_ESF_ES\", data=zdf_quest).fit()\n",
    "    res_df_ES = format_ols_results(res_ES)\n",
    "    res_df_ES['Areal'] = response\n",
    "    ols_df_ES = pd.concat([res_df_ES,ols_df_ES], axis=0, join='outer')\n",
    "    print('\\n\\n',color.BOLD, color.RED ,response, color.END, res_ES.summary())\n",
    "    \n",
    "    res_RR = smf.ols(f\"{response} ~ {covariates} + Gruppenzugehörigkeit+Kind1_ESF_RR\", data=zdf_quest).fit()\n",
    "    res_df_RR = format_ols_results(res_RR)\n",
    "    res_df_RR['Areal'] = response\n",
    "    ols_df_RR = pd.concat([res_df_RR,ols_df_RR], axis=0, join='outer')\n",
    "    print('\\n\\n',color.BOLD, color.RED ,response, color.END, res_RR.summary())\n",
    "    \n",
    "    res_SU = smf.ols(f\"{response} ~ {covariates} + Gruppenzugehörigkeit+Kind1_ESF_SU\", data=zdf_quest).fit()\n",
    "    res_df_SU = format_ols_results(res_SU)\n",
    "    res_df_SU['Areal'] = response\n",
    "    ols_df_SU = pd.concat([res_df_SU,ols_df_SU], axis=0, join='outer')\n",
    "    print('\\n\\n',color.BOLD, color.RED ,response, color.END, res_SU.summary())\n",
    "    \n",
    "    res_PS = smf.ols(f\"{response} ~ {covariates} + Gruppenzugehörigkeit+Kind1_ESF_PS\", data=zdf_quest).fit()\n",
    "    res_df_PS = format_ols_results(res_PS)\n",
    "    res_df_PS['Areal'] = response\n",
    "    ols_df_PS = pd.concat([res_df_PS,ols_df_PS], axis=0, join='outer')\n",
    "    print('\\n\\n',color.BOLD, color.RED ,response, color.END, res_PS.summary())\n",
    "ES = ols_df_ES.loc['Kind1_ESF_ES']\n",
    "RR = ols_df_RR.loc['Kind1_ESF_RR']\n",
    "SU = ols_df_SU.loc['Kind1_ESF_SU']\n",
    "PS = ols_df_PS.loc['Kind1_ESF_PS']\n",
    "#plotRoi(Cing,predictor,'Tracula White matter','(FA)',A,5) #change vektor\n",
    "\n",
    "\n",
    "#Multiple Comparison correction Bonferroni Holm\n",
    "#from statsmodels.stats.multitest import multipletests\n",
    "#print('\\n Bonferroni-Holm:', multipletests(A['pvalues'],method='holm', is_sorted=False, returnsorted=False))\n",
    "#print(round(ES,3),'\\n')\n",
    "#print(round(RR,3),'\\n')\n",
    "#print(round(SU,3),'\\n')\n",
    "#print(round(PS,3),'\\n')\n",
    "insgesamt_ESF = pd.concat([ES,RR,SU,PS])\n",
    "#Multiple Comparison correction Bonferroni Holm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print('\\n Bonferroni-Holm:', multipletests(insgesamt_ESF['pvalues'],method='holm', is_sorted=False, returnsorted=False))\n",
    "insgesamt_ESF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "predictor = ('Gruppenzugehörigkeit+Kind1_ESF_PS')\n",
    "#Gruppenzugehörigkeit+Kind1_ESF_ES\n",
    "#Gruppenzugehörigkeit+Kind1_ESF_RR\n",
    "#Gruppenzugehörigkeit+Kind1_ESF_SU\n",
    "#Gruppenzugehörigkeit+Kind1_ESF_PS\n",
    "\n",
    "for response in Cing: #change ILF,SLF,UF,CC,Cing\n",
    "    res = smf.ols(f\"{response} ~ {covariates} + {predictor}\", data=zdf_quest).fit()\n",
    "    print('\\n\\n' +\"\\033[1m\"+\"\\033[1;37;40m\" ,response,\"\\033[0m\")\n",
    "    #print(res.summary())\n",
    "    cls = LinearRegDiagnostic(res)\n",
    "    vif, fig, ax = cls()\n",
    "    #print(vif)\n",
    "    #dw= durbin_watson(res.resid)\n",
    "    #print(f\"\\n\\n Durbin-Watson: {dw}\")#1.5-2.5 good Unkorreliertheit der Residuen bzw. Fehler\n",
    "    \n",
    "    #print(OLSInfluence(res).summary_table(float_fmt='%6.3f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation Emotionregulation <a class=\"anchor\" id=\"emotion\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "ILF_ = df.filter(['lhilf_avFA_Avg' , 'lhilf_avMD_Avg' , 'rhilf_avFA_Avg', 'rhilf_avMD_Avg','Kind1_FEEL_KJ.fremd_adaptiv_gesamt','Kind1_FEEL_KJ.fremd_maladaptiv_gesamt','Kind1_FEEL_KJ.selbst_adaptiv_gesamt','Kind1_FEEL_KJ.selbst_maladaptiv_gesamt','Gruppenzugehörigkeit'])\n",
    "SLF_ = df.filter(['lhslf1_aFA_Avg', 'lhslf2_aFA_Avg', 'lhslf3_aFA_Avg', 'lhslf1_aMD_Avg', 'lhslf2_aMD_Avg','lhslf3_aMD_Avg','rhslf1_aFA_Avg', 'rhslf2_aFA_Avg', 'rhslf3_aFA_Avg', 'rhslf1_aMD_Avg', 'rhslf2_aMD_Avg', 'rhslf3_aMD_Avg','Kind1_FEEL_KJ.fremd_adaptiv_gesamt','Kind1_FEEL_KJ.fremd_maladaptiv_gesamt','Kind1_FEEL_KJ.selbst_adaptiv_gesamt','Kind1_FEEL_KJ.selbst_maladaptiv_gesamt','Gruppenzugehörigkeit'])\n",
    "UF_ = df.filter(['lhuf_avgFA_Avg','lhuf_avgMD_Avg','rhuf_avgFA_Avg', 'rhuf_avgMD_Avg','Kind1_FEEL_KJ.fremd_adaptiv_gesamt','Kind1_FEEL_KJ.fremd_maladaptiv_gesamt','Kind1_FEEL_KJ.selbst_adaptiv_gesamt','Kind1_FEEL_KJ.selbst_maladaptiv_gesamt','Gruppenzugehörigkeit'])\n",
    "CC_ = df.filter(['ccgenu_aFA_Avg', 'ccgenu_aMD_Avg','Kind1_FEEL_KJ.fremd_adaptiv_gesamt','Kind1_FEEL_KJ.fremd_maladaptiv_gesamt','Kind1_FEEL_KJ.selbst_adaptiv_gesamt','Kind1_FEEL_KJ.selbst_maladaptiv_gesamt','Gruppenzugehörigkeit'])\n",
    "Cing_ = df.filter(['rhcbd_avFA_Avg', 'rhcbd_avMD_Avg', 'rhcbv_avFA_Avg', 'rhcbv_avMD_Avg','lhcbd_avFA_Avg', 'lhcbd_avMD_Avg', 'lhcbv_avFA_Avg', 'lhcbv_avMD_Avg','Kind1_FEEL_KJ.fremd_adaptiv_gesamt','Kind1_FEEL_KJ.fremd_maladaptiv_gesamt','Kind1_FEEL_KJ.selbst_adaptiv_gesamt','Kind1_FEEL_KJ.selbst_maladaptiv_gesamt','Gruppenzugehörigkeit'])\n",
    "\n",
    "sns.set_theme(style=\"ticks\")\n",
    "sns.pairplot(ILF_, hue=\"Gruppenzugehörigkeit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "covariates = ('AlterJahreMonate  + C(Geschlecht,Treatment(reference=\"männlich\"))  + C(Standort,Treatment(reference=\"Dortmund\"))')\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "ols_df_fremd_ada = pd.DataFrame()\n",
    "ols_df_fremd_mal = pd.DataFrame()\n",
    "ols_df_selbst_ada = pd.DataFrame()\n",
    "ols_df_selbst_mal = pd.DataFrame()\n",
    "for response in Cing:   #change ILF,SLF,UF,CC,Cing\n",
    "    res_fremd_ada = smf.ols(f\"{response} ~ {covariates} + Gruppenzugehörigkeit+Kind1_FEEL_KJ_fremd_adaptiv_gesamt\", data=zdf_quest).fit()\n",
    "    res_df_fremd_ada = format_ols_results(res_fremd_ada)\n",
    "    res_df_fremd_ada['Areal'] = response\n",
    "    ols_df_fremd_ada = pd.concat([res_df_fremd_ada,ols_df_fremd_ada], axis=0, join='outer')\n",
    "    print('\\n\\n',color.BOLD, color.RED ,response, color.END, res_fremd_ada.summary())\n",
    "    \n",
    "    res_fremd_mal = smf.ols(f\"{response} ~ {covariates} + Gruppenzugehörigkeit+Kind1_FEEL_KJ_fremd_maladaptiv_gesamt\", data=zdf_quest).fit()\n",
    "    res_df_fremd_mal = format_ols_results(res_fremd_mal)\n",
    "    res_df_fremd_mal['Areal'] = response\n",
    "    ols_df_fremd_mal = pd.concat([res_df_fremd_mal,ols_df_fremd_mal], axis=0, join='outer')\n",
    "    print('\\n\\n',color.BOLD, color.RED ,response, color.END, res_fremd_mal.summary())\n",
    "    \n",
    "    res_selbst_ada = smf.ols(f\"{response} ~ {covariates} + Gruppenzugehörigkeit+Kind1_FEEL_KJ_selbst_adaptiv_gesamt\", data=zdf_quest).fit()\n",
    "    res_df_selbst_ada = format_ols_results(res_selbst_ada)\n",
    "    res_df_selbst_ada['Areal'] = response\n",
    "    ols_df_selbst_ada = pd.concat([res_df_selbst_ada,ols_df_selbst_ada], axis=0, join='outer')\n",
    "    print('\\n\\n',color.BOLD, color.RED ,response, color.END, res_selbst_ada.summary())\n",
    "    \n",
    "    res_selbst_mal = smf.ols(f\"{response} ~ {covariates} + Gruppenzugehörigkeit+Kind1_FEEL_KJ_selbst_maladaptiv_gesamt\", data=zdf_quest).fit()\n",
    "    res_df_selbst_mal = format_ols_results(res_selbst_mal)\n",
    "    res_df_selbst_mal['Areal'] = response\n",
    "    ols_df_selbst_mal = pd.concat([res_df_selbst_mal,ols_df_selbst_mal], axis=0, join='outer')\n",
    "    print('\\n\\n',color.BOLD, color.RED ,response, color.END, res_selbst_mal.summary())\n",
    "fremd_ada = ols_df_fremd_ada.loc['Kind1_FEEL_KJ_fremd_adaptiv_gesamt']\n",
    "fremd_mal = ols_df_fremd_mal.loc['Kind1_FEEL_KJ_fremd_maladaptiv_gesamt']\n",
    "selbst_ada = ols_df_selbst_ada.loc['Kind1_FEEL_KJ_selbst_adaptiv_gesamt']\n",
    "selbst_mal = ols_df_selbst_mal.loc['Kind1_FEEL_KJ_selbst_maladaptiv_gesamt']\n",
    "#plotRoi(Cing,predictor,'Tracula White matter','(FA)',A,5) #change vektor\n",
    "\n",
    "#Multiple Comparison correction Bonferroni Holm\n",
    "#from statsmodels.stats.multitest import multipletests\n",
    "#print('\\n Bonferroni-Holm:', multipletests(A['pvalues'],method='holm', is_sorted=False, returnsorted=False))\n",
    "#print(round(fremd_ada,3),'\\n')\n",
    "#print(round(fremd_mal,3),'\\n')\n",
    "#print(round(selbst_ada,3),'\\n')\n",
    "#print(round(selbst_mal,3),'\\n')\n",
    "insgesamt_FEEL = pd.concat([fremd_ada,fremd_mal,selbst_ada,selbst_mal])\n",
    "#Multiple Comparison correction Bonferroni Holm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "print('\\n Bonferroni-Holm:', multipletests(insgesamt_FEEL['pvalues'],method='holm', is_sorted=False, returnsorted=False))\n",
    "insgesamt_FEEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "predictor = ('Gruppenzugehörigkeit+Kind1_FEEL_KJ_selbst_adaptiv_gesamt')\n",
    "#Gruppenzugehörigkeit+Kind1_FEEL_KJ_fremd_adaptiv_gesamt\n",
    "#Gruppenzugehörigkeit+Kind1_FEEL_KJ_fremd_maladaptiv_gesamt\n",
    "#Gruppenzugehörigkeit+Kind1_FEEL_KJ_selbst_adaptiv_gesamt\n",
    "#Gruppenzugehörigkeit+Kind1_FEEL_KJ_selbst_maladaptiv_gesamt\n",
    "\n",
    "for response in Cing: #change ILF,SLF,UF,CC,Cing\n",
    "    res = smf.ols(f\"{response} ~ {covariates} + {predictor}\", data=zdf_quest).fit()\n",
    "    print('\\n\\n' +\"\\033[1m\"+\"\\033[1;37;40m\" ,response,\"\\033[0m\")\n",
    "    #print(res.summary())\n",
    "    cls = LinearRegDiagnostic(res)\n",
    "    vif, fig, ax = cls()\n",
    "    #print(vif)\n",
    "    #dw= durbin_watson(res.resid)\n",
    "    #print(f\"\\n\\n Durbin-Watson: {dw}\")#1.5-2.5 good Unkorreliertheit der Residuen bzw. Fehler\n",
    "    \n",
    "    #print(OLSInfluence(res).summary_table(float_fmt='%6.3f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heatmaps <a class=\"anchor\" id=\"heat\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "df = zdf_quest.filter([\"Kind1_FEEL_KJ_fremd_adaptiv_gesamt\",\"Kind1_FEEL_KJ_fremd_maladaptiv_gesamt\",\"Kind1_FEEL_KJ_selbst_adaptiv_gesamt\",\"Kind1_FEEL_KJ_selbst_maladaptiv_gesamt\",\"Kind1_EFB_GW\",\"Kind1_ESF_ES\",\"Kind1_ESF_RR\",\"Kind1_ESF_SU\",\"Kind1_ESF_PS\",\n",
    "\"SES_2\",\"AlterJahreMonate\",\"Geschlecht\",\"Standort\",\"Gruppenzugehörigkeit\",'lhslf1_aFA_Avg','lhslf2_aFA_Avg','lhslf3_aFA_Avg','lhuf_avgFA_Avg','lhilf_avFA_Avg','ccgenu_aFA_Avg','lhcbd_avFA_Avg','lhcbv_avFA_Avg','rhslf1_aFA_Avg','rhslf2_aFA_Avg',\n",
    "'rhslf3_aFA_Avg', 'rhuf_avgFA_Avg','rhilf_avFA_Avg','rhcbd_avFA_Avg','rhcbv_avFA_Avg','lhslf1_aMD_Avg', 'lhslf2_aMD_Avg','lhslf3_aMD_Avg', 'rhslf1_aMD_Avg', 'rhslf2_aMD_Avg', 'rhslf3_aMD_Avg','lhuf_avgMD_Avg', 'rhuf_avgMD_Avg',\n",
    "'lhilf_avMD_Avg' , 'rhilf_avMD_Avg','ccgenu_aMD_Avg','rhcbd_avMD_Avg', 'rhcbv_avMD_Avg', 'lhcbd_avMD_Avg', 'lhcbv_avMD_Avg'])\n",
    "\n",
    "\n",
    "df = df.rename(columns={              'lhslf1_aMD_Avg': \"l_SLF1_MD\",\n",
    "                                      'lhslf2_aMD_Avg' : \"l_SLF2_MD\",\n",
    "                                      'lhslf3_aMD_Avg' : \"l_SLF3_MD\",\n",
    "                                      'lhuf_avgMD_Avg' : \"l_UF_MD\",\n",
    "                                      'lhilf_avMD_Avg' : \"l_ILF_MD\",\n",
    "                                      'ccgenu_aMD_Avg' : \"CCgenu_MD\",\n",
    "                                      'lhcbd_avMD_Avg' : \"l_Cing_dorsal_MD\",\n",
    "                                      'lhcbv_avMD_Avg' : 'l_Cing_ventral_MD',\n",
    "                                      'rhslf1_aMD_Avg' : 'r_SLF1_MD',\n",
    "                                      'rhslf2_aMD_Avg' : 'r_SLF2_MD',\n",
    "                                      'rhslf3_aMD_Avg' : 'r_SLF3_MD',\n",
    "                                      'rhuf_avgMD_Avg' : 'r_UF_MD',\n",
    "                                      'rhilf_avMD_Avg' : 'r_ILF_MD',\n",
    "                                      'rhcbd_avMD_Avg' : 'r_Cing_dorsal_MD',\n",
    "                                      'rhcbv_avMD_Avg' : 'r_Cing_ventral_MD',              \n",
    "                                      'lhslf1_aFA_Avg' : \"l_SLF1_FA\",\n",
    "                                      'lhslf2_aFA_Avg' : \"l_SLF2_FA\",\n",
    "                                      'lhslf3_aFA_Avg' : \"l_SLF3_FA\",\n",
    "                                      'lhuf_avgFA_Avg' : \"l_UF_FA\",\n",
    "                                      'lhilf_avFA_Avg' : \"l_ILF_FA\",\n",
    "                                      'ccgenu_aFA_Avg' : \"CCgenu_FA\",\n",
    "                                      'lhcbd_avFA_Avg' : \"l_Cing_dorsal_FA\",\n",
    "                                      'lhcbv_avFA_Avg' : 'l_Cing_ventral_FA',\n",
    "                                      'rhslf1_aFA_Avg' : 'r_SLF1_FA',\n",
    "                                      'rhslf2_aFA_Avg' : 'r_SLF2_FA',\n",
    "                                      'rhslf3_aFA_Avg' : 'r_SLF3_FA',\n",
    "                                      'rhuf_avgFA_Avg' : 'r_UF_FA',\n",
    "                                      'rhilf_avFA_Avg' : 'r_ILF_FA',\n",
    "                                      'rhcbd_avFA_Avg' : 'r_Cing_dorsal_FA',\n",
    "                                      'rhcbv_avFA_Avg' : 'r_Cing_ventral_FA',\n",
    "                                      \"Kind1_FEEL_KJ_fremd_adaptiv_gesamt\" : \"FEEL_KJ_extern_adaptiv\",\n",
    "                                      \"Kind1_FEEL_KJ_fremd_maladaptiv_gesamt\" : \"FEEL_KJ_extern_maladaptiv\",\n",
    "                                      \"Kind1_FEEL_KJ_selbst_adaptiv_gesamt\" : \"FEEL_KJ_self_adaptiv\",\n",
    "                                      \"Kind1_FEEL_KJ_selbst_maladaptiv_gesamt\" : \"FEEL_KJ_self_maladaptiv\",\n",
    "                                      \"Kind1_EFB_GW\" : \"EFB_GW\",\n",
    "                                      \"Kind1_ESF_ES\" : \"ESF_ES\",\n",
    "                                      \"Kind1_ESF_RR\" : \"ESF_RR\",\n",
    "                                      \"Kind1_ESF_SU\" : \"ESF_SU\",\n",
    "                                      \"Kind1_ESF_PS\" : \"ESF_PS\",\n",
    "                                      \"SES_2\" : \"SES\",\n",
    "                                      \"AlterJahreMonate\" : \"Age\",\n",
    "                                      \"Geschlecht\" : \"Sex\",\n",
    "                                      \"Standort\" : \"Scanner_Location\",\n",
    "                                      \"Gruppenzugehörigkeit\" : \"Group\",})\n",
    "\n",
    "df = df.replace(['männlich','weiblich','Dortmund','Gießen','ja', 'nein','EG','KG'],[1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#FA\n",
    "#dataframe = df.filter([\"Group\",\"Scanner_Location\",\"Sex\",\"Age\",\"SES\",\"ESF_PS\",\"ESF_SU\",\"ESF_RR\",\"ESF_ES\",\"EFB_GW\",\"FEEL_KJ_self_maladaptiv\",\"FEEL_KJ_self_adaptiv\",\"FEEL_KJ_extern_maladaptiv\",\"FEEL_KJ_extern_adaptiv\",'r_Cing_ventral_FA','r_Cing_dorsal_FA','r_ILF_FA','r_UF_FA','r_SLF3_FA','r_SLF2_FA','r_SLF1_FA','l_Cing_ventral_FA',\"l_Cing_dorsal_FA\",\"CCgenu_FA\",\"l_ILF_FA\",\"l_UF_FA\",\"l_SLF3_FA\",\"l_SLF2_FA\",\"l_SLF1_FA\"])\n",
    "#MD\n",
    "dataframe = df.filter([\"Group\",\"Scanner Location\",\"Sex\",\"Age\",\"SES\",\"ESF_PS\",\"ESF_SU\",\"ESF_RR\",\"ESF_ES\",\"EFB_GW\",\"FEEL_KJ_self_maladaptiv\",\"FEEL_KJ_self_adaptiv\",\"FEEL_KJ_extern_maladaptiv\",\"FEEL_KJ_extern_adaptiv\",'r_Cing_ventral_MD','r_Cing_dorsal_MD','r_ILF_MD','r_UF_MD','r_SLF3_MD','r_SLF2_MD','r_SLF1_MD','l_Cing_ventral_MD',\"l_Cing_dorsal_MD\",\"CCgenu_MD\",\"l_ILF_MD\",\"l_UF_MD\",\"l_SLF3_MD\",\"l_SLF2_MD\",\"l_SLF1_MD\"])\n",
    "\n",
    "def corr_sig(dataframe=None):\n",
    "    p_matrix = np.zeros(shape=(dataframe.shape[1],dataframe.shape[1]))\n",
    "    for col in dataframe.columns:\n",
    "        for col2 in dataframe.drop(col,axis=1).columns:\n",
    "            _ , p = sc.pearsonr(dataframe[col],dataframe[col2])#pearsonr\n",
    "            p_matrix[dataframe.columns.to_list().index(col),dataframe.columns.to_list().index(col2)] = p\n",
    "    return p_matrix\n",
    "\n",
    "Bonferroni=0.05\n",
    "  \n",
    "corr = dataframe.corr()                            # get correlation\n",
    "p_values = corr_sig(dataframe)                     # get p-Value\n",
    "mask = np.invert(np.tril(p_values <= Bonferroni))    # mask - only get significant corr\n",
    "#plot_cor_matrix(corr,mask)\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(15, 12)) \n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5,annot=True, annot_kws={\"size\": 8.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Statistics - R Kernel <a class=\"anchor\" id=\"bayes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "####Change Kernel to R Kernel####\n",
    "#################################\n",
    "\n",
    "\n",
    "setwd(\"\") #set path to data\n",
    "if (!require(\"pacman\")) install.packages(\"pacman\")\n",
    " pacman::p_load(bayestestR, dplyr,fastDummies, ggplot2,ggpubr,BayesFactor,rstanarm,varBF,bayesplot, see, performance, tidyr,rcompanion,lsr)\n",
    " #check if packages are available from github install_github(...) - by using devtool packages or remotes package\n",
    "\n",
    "dfR <- read.table(\"data.csv\",sep = \",\",fileEncoding=\"UTF-8\", header=T,dec = \".\" )\n",
    "zdfR <- dfR %>% mutate(across(where(is.double), scale))\n",
    "zdfR <- dummy_cols(zdfR, select_columns = c('Gruppenzugehörigkeit','Standort','Geschlecht')) \n",
    "\n",
    "# Subset df by removing rows with NA values in specific columns\n",
    "dfR_quest <- dfR %>%\n",
    "  drop_na(Kind1_ESF.ES, \n",
    "          Kind1_FEEL_KJ.selbst_maladaptiv_gesamt, \n",
    "          Kind1_EFB.WS, \n",
    "          Kind1_FEEL_KJ.fremd_adaptiv_gesamt)\n",
    "\n",
    "zdfR_quest <- zdfR %>%\n",
    "  drop_na(Kind1_ESF.ES, \n",
    "          Kind1_FEEL_KJ.selbst_maladaptiv_gesamt, \n",
    "          Kind1_EFB.WS, \n",
    "          Kind1_FEEL_KJ.fremd_adaptiv_gesamt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "##### Sample Comparison with R <a class=\"anchor\" id=\"samplecomR\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#normal distributed -  Boxplot, QQPlot,histo with fitted curve, density distribution compare the observed distribution to what we would expect \n",
    "#Kind1_CBCL.Tsum has missing values \n",
    "defaultW <- getOption(\"warn\") \n",
    "options(warn = -1) \n",
    "\n",
    "    for(i in c(\"AlterJahreMonate\", \"SES_2\",\"Kind1_CBCL.Tsum\")) {\n",
    "        a <- ggplot(dfR, aes(x = Gruppenzugehörigkeit, y = .data[[i]])) +\n",
    "            geom_boxplot(fill=\"lightgray\") + \n",
    "            geom_dotplot(binaxis = \"y\", stackdir = \"center\", dotsize = 0.2,binwidth = 0.5)\n",
    "\n",
    "        b <- ggqqplot(dfR, x = i,\n",
    "                color = \"Gruppenzugehörigkeit\", \n",
    "                palette = c(\"#0073C2FF\", \"#FC4E07\"),\n",
    "                ggtheme = theme_pubclean())+\n",
    "                ylab(i)+\n",
    "                xlab(\"Theoretical Quantiles\")\n",
    "             \n",
    "\n",
    "        c <- gghistogram(\n",
    "                        dfR, x = i, y = \"..density..\",\n",
    "                        add = \"mean\", rug = TRUE, bins=30,\n",
    "                        fill = \"Gruppenzugehörigkeit\", palette = c(\"#00AFBB\", \"#E7B800\"),\n",
    "                        add_density = TRUE)\n",
    "        d <- ggdensity(zdfR, x = i, fill = \"lightgray\", title = i) +\n",
    "                        coord_cartesian() +\n",
    "                        stat_overlay_normal_density(color = \"red\", linetype = \"dashed\")     \n",
    "\n",
    "        options(repr.plot.width=15, repr.plot.height=10)\n",
    "        g <-     (ggarrange(a,b,c,d, \n",
    "            labels = c(\"A\", \"B\",\"C\",\"D\"),\n",
    "            ncol = 2, nrow = 2))\n",
    "        print(annotate_figure(g, top = text_grob(paste0(i), \n",
    "               color = \"red\", face = \"bold\", size = 14)))\n",
    "    }\n",
    "\n",
    "options(warn = defaultW)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#ttest; dfR_qust has only 11 EG and 36 KG \n",
    "#when using dfR only the NA in the specific variable will be ignored meaning every ttest hast a different sample size in EG and KG -> dfR_quest is the sample removed every NA in the questionaires of interest\n",
    "\n",
    "\n",
    "#    for(i in c('Kind1_FEEL_KJ.fremd_adaptiv_gesamt','Kind1_FEEL_KJ.fremd_maladaptiv_gesamt','Kind1_FEEL_KJ.selbst_adaptiv_gesamt','Kind1_FEEL_KJ.selbst_maladaptiv_gesamt','Kind1_CBCL.TEXT','Kind1_CBCL.TINT','Kind1_CBCL.Tsum', 'Kind1_EFB.NS', 'Kind1_EFB.UR','Kind1_EFB.WS','Kind1_EFB.GW','Kind1_ESF.ES','Kind1_ESF.RR','Kind1_ESF.SU','Kind1_ESF.PS','SES_2','AlterJahreMonate')) {\n",
    "#        result[[i]] <- (t.test(dfR_quest[[i]] ~ dfR_quest$Gruppenzugehörigkeit, var.equal = FALSE, alternative = \"two.sided\"))\n",
    "#        print(i);print(result[[i]])\n",
    "#    }\n",
    "result <- list()\n",
    "cohen <- list()\n",
    "    for(i in c(\"AlterJahreMonate\", \"SES_2\",\"Kind1_CBCL.Tsum\")) {\n",
    "        result[[i]] <- (t.test(dfR[[i]] ~ dfR$Gruppenzugehörigkeit, var.equal = FALSE, alternative = \"two.sided\"))\n",
    "        cohen[[i]] <- cohensD(dfR[[i]] ~ dfR$Gruppenzugehörigkeit)\n",
    "        print(i);print(result[[i]]);print(cohen[[i]]);cat(\"\\n\\n\")\n",
    "    }\n",
    "\n",
    "#Chi² or Fisher(better) for eduaction between groups\n",
    "kreuztabelle <- xtabs (~ zdfR$Gruppenzugehörigkeit + zdfR$Kind1_schule)\n",
    "n <- sum(kreuztabelle)\n",
    "erwartete_häufigkeiten <- outer (rowSums(kreuztabelle), colSums(kreuztabelle)) / n\n",
    "erwartete_häufigkeiten\n",
    "#chisq.test(zdfR$Gruppenzugehörigkeit, zdfR$Kind1_schule)\n",
    "fisher.test(zdfR$Gruppenzugehörigkeit, zdfR$Kind1_schule)\n",
    "\n",
    "kreuztabelle <- xtabs (~ zdfR$Gruppenzugehörigkeit + zdfR$Geschlecht)\n",
    "n <- sum(kreuztabelle)\n",
    "erwartete_häufigkeiten <- outer (rowSums(kreuztabelle), colSums(kreuztabelle)) / n\n",
    "erwartete_häufigkeiten\n",
    "chisq.test(zdfR$Gruppenzugehörigkeit, zdfR$Geschlecht)\n",
    "cohenW(zdfR$Gruppenzugehörigkeit, zdfR$Geschlecht)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "res <- list()\n",
    "#correlations\n",
    "for (i in c(\"Kind1_EFB.GW\",\"Kind1_ESF.ES\",\"Kind1_ESF.RR\",\"Kind1_ESF.SU\",\"Kind1_ESF.PS\",'Kind1_FEEL_KJ.selbst_adaptiv_gesamt','Kind1_FEEL_KJ.fremd_adaptiv_gesamt','Kind1_FEEL_KJ.fremd_maladaptiv_gesamt','Kind1_FEEL_KJ.selbst_adaptiv_gesamt','Kind1_FEEL_KJ.selbst_maladaptiv_gesamt')) {\n",
    "    res[[i]] <- (cor.test(zdfR_quest[[i]],zdfR_quest$SES_2, method = \"pearson\"))\n",
    "    print(i); print(res[[i]])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KG vs. EG Bayes <a class=\"anchor\" id=\"bayeseg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#Hypothesis ROIS\n",
    "ILF = c('lhilf_avFA_Avg' , 'lhilf_avMD_Avg' , 'rhilf_avFA_Avg', 'rhilf_avMD_Avg')\n",
    "SLF = c( 'lhslf1_aFA_Avg', 'lhslf2_aFA_Avg', 'lhslf3_aFA_Avg', 'lhslf1_aMD_Avg', 'lhslf2_aMD_Avg','lhslf3_aMD_Avg','rhslf1_aFA_Avg', 'rhslf2_aFA_Avg', 'rhslf3_aFA_Avg', 'rhslf1_aMD_Avg', 'rhslf2_aMD_Avg', 'rhslf3_aMD_Avg')\n",
    "UF = c('lhuf_avgFA_Avg','lhuf_avgMD_Avg','rhuf_avgFA_Avg', 'rhuf_avgMD_Avg')\n",
    "CC = c('ccgenu_aFA_Avg', 'ccgenu_aMD_Avg')\n",
    "Cing = c('rhcbd_avFA_Avg', 'rhcbd_avMD_Avg', 'rhcbv_avFA_Avg', 'rhcbv_avMD_Avg','lhcbd_avFA_Avg', 'lhcbd_avMD_Avg', 'lhcbv_avFA_Avg', 'lhcbv_avMD_Avg')\n",
    "\n",
    "#Prior\n",
    "locationpa = 0 # priors (0,1/sqrt(2)); (0,1); (0,sqrt(2)); (0,10)\n",
    "scalepa = 1\n",
    "\n",
    "model=list()\n",
    "for(i in ILF) { #change vektor ILF;SLF;UF;CC;Cing\n",
    "    set.seed(123)\n",
    "    model[[i]] <- stan_glm(paste0(i, \" ~ SES_2 + AlterJahreMonate  + Geschlecht_männlich +  Standort_Dortmund + Gruppenzugehörigkeit_EG \")  , data = zdfR, #change SES_2 \n",
    "        prior=normal(locationpa,scalepa,autoscale=FALSE),\n",
    "        prior_intercept=normal(locationpa,scalepa,autoscale=FALSE),  # priors (0,1/sqrt(2)); (0,1); (0,sqrt(2)); (0,10)\n",
    "        refresh = 0,\n",
    "        iter = 15000, #15000 iterations-5000warmups*5chains = 50000\n",
    "        warmup =5000,\n",
    "        chains =5)\n",
    "    \n",
    "    print(i);cat(\"\\n\");\n",
    "    #print(tidyMCMC(model[[i]], conf.int = TRUE, conf.method = \"HPDinterval\"))\n",
    "    #print(check_collinearity(model[[i]]))\n",
    "    print(describe_posterior(model[[i]], priors=TRUE, centrality=\"all\", ci_method=\"hdi\",test=c(\"p_direction\", \"rope\",\"bf\",\"pd\",\"ps\"), diagnostic=\"all\",ci=0.95,rope_ci=1,rope_range = \"default\"))\n",
    "    #print(prior_summary(model[[i]]))\n",
    "    #print(bf_rope(model[[i]])) #BFrope\n",
    "    #print(point_estimate(model[[i]]))\n",
    "    cat(\"\\n\\n\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#check visually Posteriors\n",
    "#change used ROI vektor to the same as above\n",
    "\n",
    "for(ROI in ILF) { #change vektor ILF;SLF;UF;CC;Cing\n",
    "  posterior <- as.matrix(model[[ROI]])\n",
    "  plot_title <- ggtitle(\"Posterior distributions\",\n",
    "                        \"with medians and 95% intervals\")\n",
    "  a <- mcmc_areas(posterior,\n",
    "            pars = c(\"SES_2\", \"AlterJahreMonate\", \"Geschlecht_männlich\",\"Gruppenzugehörigkeit_EG\"),\n",
    "            prob = 0.95) + plot_title\n",
    "\n",
    "  b <- plot(bayesfactor_parameters(model[[ROI]],parameter=\"Gruppenzugehörigkeit_EG\")) +\n",
    "    scale_color_material() +\n",
    "    scale_fill_material()    #results in a plot presenting the prior and posterior distributions for parameter. When a point null was tested, two dots represent the density of the null at the value - the ratio of their heights is the value of the Savage-Dickey Bayes factor\n",
    "                  \n",
    "  options(repr.plot.width=19, repr.plot.height=8)\n",
    "  g <- ggarrange(a,b, ncol=2,nrow=1)\n",
    "  print(annotate_figure(g, top = text_grob(paste0(ROI), \n",
    "          color = \"red\", face = \"bold\", size = 14)))\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#change used ROI vektor to the same as above\n",
    "for(i in SLF) { #change vektor ILF;SLF;UF;CC;Cing\n",
    "  resid = resid(model[[i]])\n",
    "  fit = fitted(model[[i]])\n",
    "  sresid = resid/sd(resid)\n",
    "\n",
    "  options(repr.plot.width=22, repr.plot.height=18)\n",
    "  #non-linearity, unequal error variances, and outliers (e.g. residual vs fitted plot)\n",
    "  a <- ggplot(data=NULL,mapping=aes(x=fit,y=resid)) +\n",
    "              geom_point(shape=1) +\n",
    "              geom_hline(yintercept=0,linetype=\"dashed\") +\n",
    "              geom_smooth(color=\"red\",linetype = \"dashed\",linewidth=0.5)+\n",
    "              ylab(\"Residuals\")+\n",
    "              xlab(\"Fitted valus\")+\n",
    "              ggtitle(\"Residual vs. Fitted\")\n",
    "\n",
    "\n",
    "  #qqPlot of residuals (should be normal distributed)\n",
    "  b <- ggplot(data=NULL, aes(sample = sresid)) +\n",
    "              ggtitle(\"Normal Q-Q\")+  \n",
    "              ylab(\"Standardized residuals\")+\n",
    "              xlab(\"Theoretical Quantiles\")+\n",
    "              geom_qq(shape=1) +\n",
    "              geom_qq_line(linetype = \"dashed\",color=\"red\")\n",
    "\n",
    "  #Posterior predictive check (makes model sense to explain data)\n",
    "  c <- pp_check(model[[i]], nreps=100) + xlab(paste0(i)) + ggtitle(\"Posterior predictive check\") + theme(plot.title = element_text(hjust = 0.06))\n",
    "\n",
    "  #this compares the posterior estimate for each parameter against the associated prior. \n",
    "  #If the spread of the priors is small relative to the posterior, then it is likely that the priors are too influential.\n",
    "  d <- posterior_vs_prior(model[[i]], color_by = \"vs\", group_by = TRUE, \n",
    "                          facet_args = list(scales = \"free_y\")) + ggtitle(\"Posterior vs. Prior\")\n",
    "\n",
    "  color_scheme_set(\"mix-blue-red\")\n",
    "  #take a look at the posteriors for each chain and the trace \n",
    "  #Trace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space\n",
    "  e <- mcmc_combo(model[[i]],\n",
    "                  combo = c(\"dens_overlay\", \"trace\"), \n",
    "                  gg_theme = ggplot2::theme_gray()) \n",
    "\n",
    "  #autocorrelation check \n",
    "  f <- mcmc_acf(model[[i]])\n",
    " \n",
    "  g <- (ggarrange(a,b,c,d,e,f, \n",
    "                  labels = c(\"A\", \"B\",\"C\",\"D\",\"E\",\"F\"),\n",
    "                  ncol = 2, nrow = 3))\n",
    "\n",
    "  print(annotate_figure(g, top = text_grob(paste0(i), \n",
    "                        color = \"red\", face = \"bold\", size = 14)))\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation parenting skill Bayes <a class=\"anchor\" id=\"corrbayes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#Hypothesis ROIS\n",
    "ILF = c('lhilf_avFA_Avg' , 'lhilf_avMD_Avg' , 'rhilf_avFA_Avg', 'rhilf_avMD_Avg')\n",
    "SLF = c( 'lhslf1_aFA_Avg', 'lhslf2_aFA_Avg', 'lhslf3_aFA_Avg', 'lhslf1_aMD_Avg', 'lhslf2_aMD_Avg','lhslf3_aMD_Avg','rhslf1_aFA_Avg', 'rhslf2_aFA_Avg', 'rhslf3_aFA_Avg', 'rhslf1_aMD_Avg', 'rhslf2_aMD_Avg', 'rhslf3_aMD_Avg')\n",
    "UF = c('lhuf_avgFA_Avg','lhuf_avgMD_Avg','rhuf_avgFA_Avg', 'rhuf_avgMD_Avg')\n",
    "CC = c('ccgenu_aFA_Avg', 'ccgenu_aMD_Avg')\n",
    "Cing = c('rhcbd_avFA_Avg', 'rhcbd_avMD_Avg', 'rhcbv_avFA_Avg', 'rhcbv_avMD_Avg','lhcbd_avFA_Avg', 'lhcbd_avMD_Avg', 'lhcbv_avFA_Avg', 'lhcbv_avMD_Avg')\n",
    "\n",
    "# Remove rows where the 'ID' column is 'sub-070' the one making problems above\n",
    "zdfR_quest_without <- zdfR_quest[zdfR_quest$ID != 'sub-070',]\n",
    "\n",
    "#Prior\n",
    "locationpa = 0 # priors (0,1/sqrt(2)); (0,1); (0,sqrt(2)); (0,10)\n",
    "scalepa = 1\n",
    "\n",
    "model=list()\n",
    "for(i in SLF) { #change vektor ILF;SLF;UF;CC;Cing\n",
    "    set.seed(123)\n",
    "    model[[i]] <- stan_glm(paste0(i, \" ~ AlterJahreMonate  + Geschlecht_männlich +  Standort_Dortmund + Gruppenzugehörigkeit_EG + Kind1_EFB.GW \")  , data = zdfR_quest, #change SES_2 \n",
    "        prior=normal(locationpa,scalepa,autoscale=FALSE),\n",
    "        prior_intercept=normal(locationpa,scalepa,autoscale=FALSE),  # priors (0,1/sqrt(2)); (0,1); (0,sqrt(2)); (0,10)\n",
    "        refresh = 0,\n",
    "        iter = 15000, #15000 iterations-5000warmups*5chains = 50000\n",
    "        warmup =5000,\n",
    "        chains =5)\n",
    "    \n",
    "    print(i);cat(\"\\n\");\n",
    "    #print(tidyMCMC(model[[i]], conf.int = TRUE, conf.method = \"HPDinterval\"))\n",
    "    #print(check_collinearity(model[[i]]))\n",
    "    print(describe_posterior(model[[i]], priors=TRUE, centrality=\"all\", ci_method=\"hdi\",test=c(\"p_direction\", \"rope\",\"bf\",\"pd\",\"ps\"), diagnostic=\"all\",ci=0.95,rope_ci=1,rope_range = \"default\"))\n",
    "    #print(prior_summary(model[[i]]))\n",
    "    #print(bf_rope(model[[i]])) #BFrope\n",
    "    #print(point_estimate(model[[i]]))\n",
    "    cat(\"\\n\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#check visually Posteriors\n",
    "#change used ROI vektor to the same as above\n",
    "\n",
    "for(ROI in ILF) { #change vektor ILF;SLF;UF;CC;Cing\n",
    "  posterior <- as.matrix(model[[ROI]])\n",
    "  plot_title <- ggtitle(\"Posterior distributions\",\n",
    "                        \"with medians and 95% intervals\")\n",
    "  a <- mcmc_areas(posterior,\n",
    "            pars = c(\"SES_2\", \"AlterJahreMonate\", \"Geschlecht_männlich\",\"Gruppenzugehörigkeit_EG\", \"Kind1_EFB.GW\"),\n",
    "            prob = 0.95) + plot_title\n",
    "\n",
    "  b <- plot(bayesfactor_parameters(model[[ROI]],parameter=\"Kind1_EFB.GW\")) +\n",
    "    scale_color_material() +\n",
    "    scale_fill_material()    #results in a plot presenting the prior and posterior distributions for parameter. When a point null was tested, two dots represent the density of the null at the value - the ratio of their heights is the value of the Savage-Dickey Bayes factor\n",
    "                  \n",
    "  options(repr.plot.width=19, repr.plot.height=8)\n",
    "  g <- ggarrange(a,b, ncol=2,nrow=1)\n",
    "  print(annotate_figure(g, top = text_grob(paste0(ROI), \n",
    "          color = \"red\", face = \"bold\", size = 14)))\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#change used ROI vektor to the same as above\n",
    "for(i in ILF) { #change vektor ILF;SLF;UF;CC;Cing\n",
    "  resid = resid(model[[i]])\n",
    "  fit = fitted(model[[i]])\n",
    "  sresid = resid/sd(resid)\n",
    "\n",
    "  options(repr.plot.width=22, repr.plot.height=18)\n",
    "  #non-linearity, unequal error variances, and outliers (e.g. residual vs fitted plot)\n",
    "  a <- ggplot(data=NULL,mapping=aes(x=fit,y=resid)) +\n",
    "              geom_point(shape=1) +\n",
    "              geom_hline(yintercept=0,linetype=\"dashed\") +\n",
    "              geom_smooth(color=\"red\",linetype = \"dashed\",linewidth=0.5)+\n",
    "              ylab(\"Residuals\")+\n",
    "              xlab(\"Fitted valus\")+\n",
    "              ggtitle(\"Residual vs. Fitted\")\n",
    "\n",
    "\n",
    "  #qqPlot of residuals (should be normal distributed)\n",
    "  b <- ggplot(data=NULL, aes(sample = sresid)) +\n",
    "              ggtitle(\"Normal Q-Q\")+  \n",
    "              ylab(\"Standardized residuals\")+\n",
    "              xlab(\"Theoretical Quantiles\")+\n",
    "              geom_qq(shape=1) +\n",
    "              geom_qq_line(linetype = \"dashed\",color=\"red\")\n",
    "\n",
    "  #Posterior predictive check (makes model sense to explain data)\n",
    "  c <- pp_check(model[[i]], nreps=100) + xlab(paste0(i)) + ggtitle(\"Posterior predictive check\") + theme(plot.title = element_text(hjust = 0.06))\n",
    "\n",
    "  #this compares the posterior estimate for each parameter against the associated prior. \n",
    "  #If the spread of the priors is small relative to the posterior, then it is likely that the priors are too influential.\n",
    "  d <- posterior_vs_prior(model[[i]], color_by = \"vs\", group_by = TRUE, \n",
    "                          facet_args = list(scales = \"free_y\")) + ggtitle(\"Posterior vs. Prior\")\n",
    "\n",
    "  color_scheme_set(\"mix-blue-red\")\n",
    "  #take a look at the posteriors for each chain and the trace \n",
    "  #Trace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space\n",
    "  e <- mcmc_combo(model[[i]],\n",
    "                  combo = c(\"dens_overlay\", \"trace\"), \n",
    "                  gg_theme = ggplot2::theme_gray()) \n",
    "\n",
    "  #autocorrelation check \n",
    "  f <- mcmc_acf(model[[i]])\n",
    " \n",
    "  g <- (ggarrange(a,b,c,d,e,f, \n",
    "                  labels = c(\"A\", \"B\",\"C\",\"D\",\"E\",\"F\"),\n",
    "                  ncol = 2, nrow = 3))\n",
    "\n",
    "  print(annotate_figure(g, top = text_grob(paste0(i), \n",
    "                        color = \"red\", face = \"bold\", size = 14)))\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation parenting stress Bayes <a class=\"anchor\" id=\"corrstressbayes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#Hypothesis ROIS\n",
    "ILF = c('lhilf_avFA_Avg' , 'lhilf_avMD_Avg' , 'rhilf_avFA_Avg', 'rhilf_avMD_Avg')\n",
    "SLF = c( 'lhslf1_aFA_Avg', 'lhslf2_aFA_Avg', 'lhslf3_aFA_Avg', 'lhslf1_aMD_Avg', 'lhslf2_aMD_Avg','lhslf3_aMD_Avg','rhslf1_aFA_Avg', 'rhslf2_aFA_Avg', 'rhslf3_aFA_Avg', 'rhslf1_aMD_Avg', 'rhslf2_aMD_Avg', 'rhslf3_aMD_Avg')\n",
    "UF = c('lhuf_avgFA_Avg','lhuf_avgMD_Avg','rhuf_avgFA_Avg', 'rhuf_avgMD_Avg')\n",
    "CC = c('ccgenu_aFA_Avg', 'ccgenu_aMD_Avg')\n",
    "Cing = c('rhcbd_avFA_Avg', 'rhcbd_avMD_Avg', 'rhcbv_avFA_Avg', 'rhcbv_avMD_Avg','lhcbd_avFA_Avg', 'lhcbd_avMD_Avg', 'lhcbv_avFA_Avg', 'lhcbv_avMD_Avg')\n",
    "\n",
    "# Remove rows where the 'ID' column is 'sub-070' the one making problems above\n",
    "zdfR_quest_without <- zdfR_quest[zdfR_quest$ID != 'sub-070',]\n",
    "\n",
    "\n",
    "#ESF - Predictor of interest\n",
    "ESF = c(\"Kind1_ESF.PS\") #'Kind1_ESF.ES','Kind1_ESF.RR','Kind1_ESF.SU','Kind1_ESF.PS'\n",
    "\n",
    "#Prior\n",
    "locationpa = 0 # priors (0,1/sqrt(2)); (0,1); (0,sqrt(2)); (0,10)\n",
    "scalepa = 1\n",
    "\n",
    "model=list()\n",
    "for(i in ILF) { #change vektor ILF;SLF;UF;CC;Cing\n",
    "    set.seed(123)\n",
    "    model[[i]] <- stan_glm(paste0(i, \" ~ SES_2 + AlterJahreMonate  + Geschlecht_männlich +  Standort_Dortmund + Gruppenzugehörigkeit_EG + \", ESF)  , data = zdfR_quest, #change SES_2\n",
    "        prior=normal(locationpa,scalepa,autoscale=FALSE),\n",
    "        prior_intercept=normal(locationpa,scalepa,autoscale=FALSE),  # priors (0,1/sqrt(2)); (0,1); (0,sqrt(2)); (0,10)\n",
    "        refresh = 0,\n",
    "        iter = 15000, #15000 iterations-5000warmups*5chains = 50000\n",
    "        warmup =5000,\n",
    "        chains =5)\n",
    "    \n",
    "    print(i);cat(\"\\n\");\n",
    "    #print(tidyMCMC(model[[i]], conf.int = TRUE, conf.method = \"HPDinterval\"))\n",
    "    #print(check_collinearity(model[[i]]))\n",
    "    print(describe_posterior(model[[i]], priors=TRUE, centrality=\"all\", ci_method=\"hdi\",test=c(\"p_direction\", \"rope\",\"bf\",\"pd\",\"ps\"), diagnostic=\"all\",ci=0.95,rope_ci=1,rope_range = \"default\"))\n",
    "    #print(prior_summary(model[[i]]))\n",
    "    #print(bf_rope(model[[i]])) #BFrope\n",
    "    #print(point_estimate(model[[i]]))\n",
    "    cat(\"\\n\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#check visually Posteriors\n",
    "#change used ROI vektor to the same as above\n",
    "\n",
    "for(ROI in ILF) { #change vektor ILF;SLF;UF;CC;Cing\n",
    "  posterior <- as.matrix(model[[ROI]])\n",
    "  plot_title <- ggtitle(\"Posterior distributions\",\n",
    "                        \"with medians and 95% intervals\")\n",
    "  a <- mcmc_areas(posterior,\n",
    "            pars = c(\"SES_2\", \"AlterJahreMonate\", \"Geschlecht_männlich\",\"Gruppenzugehörigkeit_EG\", ESF),\n",
    "            prob = 0.95) + plot_title\n",
    "\n",
    "  b <- plot(bayesfactor_parameters(model[[ROI]],parameter=ESF)) +\n",
    "    scale_color_material() +\n",
    "    scale_fill_material()    #results in a plot presenting the prior and posterior distributions for parameter. When a point null was tested, two dots represent the density of the null at the value - the ratio of their heights is the value of the Savage-Dickey Bayes factor\n",
    "                  \n",
    "  options(repr.plot.width=19, repr.plot.height=8)\n",
    "  g <- ggarrange(a,b, ncol=2,nrow=1)\n",
    "  print(annotate_figure(g, top = text_grob(paste0(ROI), \n",
    "          color = \"red\", face = \"bold\", size = 14)))\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#change used ROI vektor to the same as above\n",
    "for(i in ILF) { #change vektor ILF;SLF;UF;CC;Cing\n",
    "  resid = resid(model[[i]])\n",
    "  fit = fitted(model[[i]])\n",
    "  sresid = resid/sd(resid)\n",
    "\n",
    "  options(repr.plot.width=22, repr.plot.height=18)\n",
    "  #non-linearity, unequal error variances, and outliers (e.g. residual vs fitted plot)\n",
    "  a <- ggplot(data=NULL,mapping=aes(x=fit,y=resid)) +\n",
    "              geom_point(shape=1) +\n",
    "              geom_hline(yintercept=0,linetype=\"dashed\") +\n",
    "              geom_smooth(color=\"red\",linetype = \"dashed\",linewidth=0.5)+\n",
    "              ylab(\"Residuals\")+\n",
    "              xlab(\"Fitted valus\")+\n",
    "              ggtitle(\"Residual vs. Fitted\")\n",
    "\n",
    "\n",
    "  #qqPlot of residuals (should be normal distributed)\n",
    "  b <- ggplot(data=NULL, aes(sample = sresid)) +\n",
    "              ggtitle(\"Normal Q-Q\")+  \n",
    "              ylab(\"Standardized residuals\")+\n",
    "              xlab(\"Theoretical Quantiles\")+\n",
    "              geom_qq(shape=1) +\n",
    "              geom_qq_line(linetype = \"dashed\",color=\"red\")\n",
    "\n",
    "  #Posterior predictive check (makes model sense to explain data)\n",
    "  c <- pp_check(model[[i]], nreps=100) + xlab(paste0(i)) + ggtitle(\"Posterior predictive check\") + theme(plot.title = element_text(hjust = 0.06))\n",
    "\n",
    "  #this compares the posterior estimate for each parameter against the associated prior. \n",
    "  #If the spread of the priors is small relative to the posterior, then it is likely that the priors are too influential.\n",
    "  d <- posterior_vs_prior(model[[i]], color_by = \"vs\", group_by = TRUE, \n",
    "                          facet_args = list(scales = \"free_y\")) + ggtitle(\"Posterior vs. Prior\")\n",
    "\n",
    "  color_scheme_set(\"mix-blue-red\")\n",
    "  #take a look at the posteriors for each chain and the trace \n",
    "  #Trace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space\n",
    "  e <- mcmc_combo(model[[i]],\n",
    "                  combo = c(\"dens_overlay\", \"trace\"), \n",
    "                  gg_theme = ggplot2::theme_gray()) \n",
    "\n",
    "  #autocorrelation check \n",
    "  f <- mcmc_acf(model[[i]])\n",
    " \n",
    "  g <- (ggarrange(a,b,c,d,e,f, \n",
    "                  labels = c(\"A\", \"B\",\"C\",\"D\",\"E\",\"F\"),\n",
    "                  ncol = 2, nrow = 3))\n",
    "\n",
    "  print(annotate_figure(g, top = text_grob(paste0(i), \n",
    "                        color = \"red\", face = \"bold\", size = 14)))\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation Emotionregulation Bayes <a class=\"anchor\" id=\"emotionbayes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#Hypothesis ROIS\n",
    "ILF = c('lhilf_avFA_Avg' , 'lhilf_avMD_Avg' , 'rhilf_avFA_Avg', 'rhilf_avMD_Avg')\n",
    "SLF = c( 'lhslf1_aFA_Avg', 'lhslf2_aFA_Avg', 'lhslf3_aFA_Avg', 'lhslf1_aMD_Avg', 'lhslf2_aMD_Avg','lhslf3_aMD_Avg','rhslf1_aFA_Avg', 'rhslf2_aFA_Avg', 'rhslf3_aFA_Avg', 'rhslf1_aMD_Avg', 'rhslf2_aMD_Avg', 'rhslf3_aMD_Avg')\n",
    "UF = c('lhuf_avgFA_Avg','lhuf_avgMD_Avg','rhuf_avgFA_Avg', 'rhuf_avgMD_Avg')\n",
    "CC = c('ccgenu_aFA_Avg', 'ccgenu_aMD_Avg')\n",
    "Cing = c('rhcbd_avFA_Avg', 'rhcbd_avMD_Avg', 'rhcbv_avFA_Avg', 'rhcbv_avMD_Avg','lhcbd_avFA_Avg', 'lhcbd_avMD_Avg', 'lhcbv_avFA_Avg', 'lhcbv_avMD_Avg')\n",
    "\n",
    "# Remove rows where the 'ID' column is 'sub-070' the one making problems above\n",
    "zdfR_quest_without <- zdfR_quest[zdfR_quest$ID != 'sub-070',]\n",
    "\n",
    "\n",
    "#Emot - Predictor of interest\n",
    "Emot = c('Kind1_FEEL_KJ.selbst_adaptiv_gesamt') #'Kind1_FEEL_KJ.fremd_adaptiv_gesamt','Kind1_FEEL_KJ.fremd_maladaptiv_gesamt','Kind1_FEEL_KJ.selbst_adaptiv_gesamt','Kind1_FEEL_KJ.selbst_maladaptiv_gesamt'\n",
    "\n",
    "#Prior\n",
    "locationpa = 0 # priors (0,1/sqrt(2)); (0,1); (0,sqrt(2)); (0,10)\n",
    "scalepa = 1\n",
    "\n",
    "model=list()\n",
    "for(i in ILF) { #change vektor ILF;SLF;UF;CC;Cing\n",
    "    set.seed(123)\n",
    "    model[[i]] <- stan_glm(paste0(i, \" ~ SES_2 + AlterJahreMonate  + Geschlecht_männlich +  Standort_Dortmund + Gruppenzugehörigkeit_EG + \", Emot)  , data = zdfR_quest, #change SES_2\n",
    "        prior=normal(locationpa,scalepa,autoscale=FALSE),\n",
    "        prior_intercept=normal(locationpa,scalepa,autoscale=FALSE),  # priors (0,1/sqrt(2)); (0,1); (0,sqrt(2)); (0,10)\n",
    "        refresh = 0,\n",
    "        iter = 15000, #15000 iterations-5000warmups*5chains = 50000\n",
    "        warmup =5000,\n",
    "        chains =5)\n",
    "    \n",
    "    print(i);cat(\"\\n\");\n",
    "    #print(tidyMCMC(model[[i]], conf.int = TRUE, conf.method = \"HPDinterval\"))\n",
    "    #print(check_collinearity(model[[i]]))\n",
    "    print(describe_posterior(model[[i]], priors=TRUE, centrality=\"all\", ci_method=\"hdi\",test=c(\"p_direction\", \"rope\",\"bf\",\"pd\",\"ps\"), diagnostic=\"all\",ci=0.95,rope_ci=1,rope_range = \"default\"))\n",
    "    #print(prior_summary(model[[i]]))\n",
    "    #print(bf_rope(model[[i]])) #BFrope\n",
    "    #print(point_estimate(model[[i]]))\n",
    "    cat(\"\\n\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#check visually Posteriors\n",
    "#change used ROI vektor to the same as above\n",
    "\n",
    "for(ROI in ILF) { #change vektor ILF;SLF;UF;CC;Cing\n",
    "  posterior <- as.matrix(model[[ROI]])\n",
    "  plot_title <- ggtitle(\"Posterior distributions\",\n",
    "                        \"with medians and 95% intervals\")\n",
    "  a <- mcmc_areas(posterior,\n",
    "            pars = c(\"SES_2\", \"AlterJahreMonate\", \"Geschlecht_männlich\",\"Gruppenzugehörigkeit_EG\", Emot),\n",
    "            prob = 0.95) + plot_title\n",
    "\n",
    "  b <- plot(bayesfactor_parameters(model[[ROI]],parameter=Emot)) +\n",
    "    scale_color_material() +\n",
    "    scale_fill_material()    #results in a plot presenting the prior and posterior distributions for parameter. When a point null was tested, two dots represent the density of the null at the value - the ratio of their heights is the value of the Savage-Dickey Bayes factor\n",
    "                  \n",
    "  options(repr.plot.width=19, repr.plot.height=8)\n",
    "  g <- ggarrange(a,b, ncol=2,nrow=1)\n",
    "  print(annotate_figure(g, top = text_grob(paste0(ROI), \n",
    "          color = \"red\", face = \"bold\", size = 14)))\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#change used ROI vektor to the same as above\n",
    "for(i in ILF) { #change vektor ILF;SLF;UF;CC;Cing\n",
    "  resid = resid(model[[i]])\n",
    "  fit = fitted(model[[i]])\n",
    "  sresid = resid/sd(resid)\n",
    "\n",
    "  options(repr.plot.width=22, repr.plot.height=18)\n",
    "  #non-linearity, unequal error variances, and outliers (e.g. residual vs fitted plot)\n",
    "  a <- ggplot(data=NULL,mapping=aes(x=fit,y=resid)) +\n",
    "              geom_point(shape=1) +\n",
    "              geom_hline(yintercept=0,linetype=\"dashed\") +\n",
    "              geom_smooth(color=\"red\",linetype = \"dashed\",linewidth=0.5)+\n",
    "              ylab(\"Residuals\")+\n",
    "              xlab(\"Fitted valus\")+\n",
    "              ggtitle(\"Residual vs. Fitted\")\n",
    "\n",
    "\n",
    "  #qqPlot of residuals (should be normal distributed)\n",
    "  b <- ggplot(data=NULL, aes(sample = sresid)) +\n",
    "              ggtitle(\"Normal Q-Q\")+  \n",
    "              ylab(\"Standardized residuals\")+\n",
    "              xlab(\"Theoretical Quantiles\")+\n",
    "              geom_qq(shape=1) +\n",
    "              geom_qq_line(linetype = \"dashed\",color=\"red\")\n",
    "\n",
    "  #Posterior predictive check (makes model sense to explain data)\n",
    "  c <- pp_check(model[[i]], nreps=100) + xlab(paste0(i)) + ggtitle(\"Posterior predictive check\") + theme(plot.title = element_text(hjust = 0.06))\n",
    "\n",
    "  #this compares the posterior estimate for each parameter against the associated prior. \n",
    "  #If the spread of the priors is small relative to the posterior, then it is likely that the priors are too influential.\n",
    "  d <- posterior_vs_prior(model[[i]], color_by = \"vs\", group_by = TRUE, \n",
    "                          facet_args = list(scales = \"free_y\")) + ggtitle(\"Posterior vs. Prior\")\n",
    "\n",
    "  color_scheme_set(\"mix-blue-red\")\n",
    "  #take a look at the posteriors for each chain and the trace \n",
    "  #Trace plots show no evidence that the chains have not reasonably traversed the entire multidimensional parameter space\n",
    "  e <- mcmc_combo(model[[i]],\n",
    "                  combo = c(\"dens_overlay\", \"trace\"), \n",
    "                  gg_theme = ggplot2::theme_gray()) \n",
    "\n",
    "  #autocorrelation check \n",
    "  f <- mcmc_acf(model[[i]])\n",
    " \n",
    "  g <- (ggarrange(a,b,c,d,e,f, \n",
    "                  labels = c(\"A\", \"B\",\"C\",\"D\",\"E\",\"F\"),\n",
    "                  ncol = 2, nrow = 3))\n",
    "\n",
    "  print(annotate_figure(g, top = text_grob(paste0(i), \n",
    "                        color = \"red\", face = \"bold\", size = 14)))\n",
    "\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
